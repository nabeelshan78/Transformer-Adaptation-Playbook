
The Transfer Learning Playbook: Pre-training and Fine-tuning Transformers
Objective: This notebook provides a hands-on exploration of transfer learning in NLP. We build a Transformer model from scratch in PyTorch, pre-train it on a general news classification task (AG News), and then fine-tune it for sentiment analysis on movie reviews (IMDB), comparing different adaptation strategies.

Experimental Setup
We investigate three distinct training strategies to evaluate the effectiveness of transfer learning:

From Scratch: A baseline Transformer is trained only on the target IMDB dataset.
Full Fine-Tuning: The entire model, pre-trained on AG News, is re-trained on the IMDB dataset.
Linear Probing: Only the final classification layer of the pre-trained model is re-trained, keeping all Transformer layers frozen.
Key Findings & Conclusion
The results clearly demonstrate the power of transfer learning for achieving superior performance.

Training Strategy	Test Accuracy (IMDB)	Key Takeaway
Trained from Scratch	~83%	Decent, but requires significant task-specific data.
Full Fine-Tuning	~86%	Best performance, showing that adapting all layers is highly effective.
Linear Probing	~64%	Fastest to train, but a major drop in performance.
Conclusion: Full fine-tuning provides a significant accuracy boost over training from scratch by leveraging knowledge from a pre-training task. This confirms that transfer learning is a critical technique for building high-performing NLP models.

Model: Custom Transformer Encoder with GloVe Embeddings
Framework: PyTorch, TorchText
Datasets: AG News (Source), IMDB Movie Reviews (Target)
# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install torchdata==0.7.1
# !pip install pandas
# !pip install matplotlib==3.9.0 scikit-learn==1.5.0
# !pip install numpy==1.26.0
# !pip install portalocker
import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
from torch.nn.utils.rnn import pad_sequence
from torchtext.datasets import AG_NEWS

import matplotlib.pyplot as plt
from tqdm import tqdm

from urllib.request import urlopen
import pickle
import tarfile
import io
import tempfile
import math
import os
import time

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
 
 
def plot(COST, ACC):

    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout() 

    plt.show()
def save_list_to_file(lst, filename):
    """
    Save a list to a file using pickle serialization.
    """
    with open(filename, 'wb') as file:
        pickle.dump(lst, file)

def load_list_from_file(filename):
    """
    Load a list from a file using pickle deserialization.
    """
    with open(filename, 'rb') as file:
        loaded_list = pickle.load(file)
    return loaded_list
Positional encodings
class PositionalEncoding(nn.Module):
    """
    Implements the sinusoidal positional encoding layer for Transformer models.
    """

    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create a matrix of shape (max_len, d_model) to hold all positional encodings.
        pe = torch.zeros(max_len, d_model)
        # Shape: [max_len, 1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # `div_term` calculates the denominator part of the formula: 1 / (10000^(2i/d_model)).
        # The use of exp(log(...)) is a clever trick for numerical stability.
        # `2i` is represented by `torch.arange(0, d_model, 2)`.
        # Shape: [d_model / 2]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Broadcasting `position` [max_len, 1] with `div_term` [d_model/2]
        # results in a matrix of shape [max_len, d_model/2].
        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices
        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices

        # New shape: [1, max_len, d_model]
        pe = pe.unsqueeze(0)

        # `register_buffer` makes `pe` a part of the model's state, but not a parameter
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Adds positional encoding to the input tensor.

        Args:
            x (torch.Tensor): The input tensor of word embeddings.
                              Shape: [batch_size, seq_len, d_model]

        Returns:
            torch.Tensor: The input tensor with positional encodings added.
                          Shape: [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
Import IMDB dataset
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')
tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))
tempdir = tempfile.TemporaryDirectory()
tar.extractall(tempdir.name)
tar.close()
os.listdir(tempdir.name)
['imdb_dataset']
 
class IMDBDataset(Dataset):
    def __init__(self, root_dir, train=True):
        """
        root_dir: The base directory of the IMDB dataset.
        train: A boolean flag indicating whether to use training or test data.
        """
        self.root_dir = os.path.join(root_dir, "train" if train else "test")
        self.neg_files = [os.path.join(self.root_dir, "neg", f) for f in os.listdir(os.path.join(self.root_dir, "neg")) if f.endswith('.txt')]
        self.pos_files = [os.path.join(self.root_dir, "pos", f) for f in os.listdir(os.path.join(self.root_dir, "pos")) if f.endswith('.txt')]
        self.files = self.neg_files + self.pos_files
        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)
        self.pos_inx = len(self.pos_files)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        return label, content
The following code uses the IMDBDataset class previously defined to create iterators for the train and test datasets.

root_dir = tempdir.name + '/' + 'imdb_dataset'
root_dir
'/tmp/tmp9yd0ecbc/imdb_dataset'
train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data
test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data
start = train_iter.pos_inx
for i in range(-2, 2):
    print(train_iter[start+i])
    print('='*100)
(0, 'In a far away Galaxy is a planet called Ceta. It\'s native people worship cats. But the dog people wage war upon these feline loving people and they have no choice but to go to Earth and grind people up for food. This is one of the stupidest f#@k!ng ideas for a movie I\'ve seen. Leave it to Ted Mikels to make a movie more incompetent than the already low standard he set in previous films. It\'s like he enjoying playing in a celluloid game of Limbo. How low can he go? The only losers in the scenario are US the viewer. Mr. Mikels and his silly little handlebar mustache actually has people who STILL buy this crap.<br /><br />My Grade: F <br /><br />DVD Extras: Commentary by Ted Mikels; the Story behind the Making of (9 and a half minutes); 17 minutes, 15 seconds of Behind the scenes footage; Ted Mikels filmography; and Trailers for "The Worm Eaters" "Girl in Gold Boots", "the Doll Squad", "Ten Violent Women" (featuring nudity), "Blood Orgy of the She Devils", & "the Corpse Grinders"')
====================================================================================================
(0, 'Adrianne, should really get a life-without Mr. "Brady". She nauseates me, and has been one of the main reasons why I know longer tune in to the show. It\'s pretty brainless show, and every little argument or disagreement seems to be put under the scope and analyzed to death. This makes them look/sound they are anything but ready for marriage, and yet, I know these disagreements are all part of life. I guess to some people this is entertainment. If this happens to fall into next season I will feel sorry for anyone who has nothing better to do with their life but watch this trash. Though I would not be terribly surprised. can\'t even stand the commercials for this show anymore! I hope they\'re getting enough money to constantly embarrass themselves in front of a camera week after week. However, the "A" girl has one heck of great butt!')
====================================================================================================
(1, 'I was lucky enough to see this at a pre-screening last night (Oct. 20) and I was incredibly surprised by the wonderful plot and genuinely heart felt acting.<br /><br />While the plot is not particularly complicated or exceptionally new, the story unfolds in a way that feels fresh, unique, and distinctly "indy" in style. It isn\'t something that can easily be compared to films of the past, it\'s a unique take on a sort of classic middle-aged depressed love story.<br /><br />I was particularly struck by the casting of the film. Down to every last extra in the family, it was a beautiful and talented cast. The three daughters did a wonderful job, the talent was evenly dispersed between them and none of them "out-shone" the other two.<br /><br />It was truly a delightful film, appropriate for all ages and laugh out loud funny while also being truly touching and heart warming. It was a wonderful break from the sex jokes and nudity of recent films.')
====================================================================================================
(1, 'Unfortunately, this movie never made it to DVD. I saw it when it was first released to the theaters in 1983, and then again when the VHS was released in 1992. When I recently saw a VHS copy at a flea market, I immediately bought it. I was not disappointed. First, the obvious: Claudia Ohana is beautiful and a joy to behold. But then, the film takes you into an unreal world where you have to reflect on your values and decide what is really important to you. The movie is about a lot of things. It is about how the World Bank and large corporations exploit and enslave developing countries with their capitalist schemes to force them into a debt that they can never repay. It is about how our economic system exploits us by forcing us into debt with credit cards, mortgage, and car payment. It is about trying to save an innocence that maybe we have never really had and maybe we cannot really save. It is about good and evil and about how hard it is sometimes to tell one from the other. It raises a lot of questions but does not give answers. I think sometimes this is good. For this reason, this is a film really worth saving and seeing.')
====================================================================================================
 
imdb_label = {0: "negative review", 1: "positive review"}
num_class = len(set([label for (label, text) in train_iter]))
num_class
2
tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    """Yield tokens for each data sample."""
    for _, text in data_iter:
        yield tokenizer(text)
The following code loads a pretrained word embedding model called GloVe into a variable called glove_embedding.

glove_embedding = GloVe(name="6B", dim=100)
The following code builds a vocabulary object from a pretrained GloVe word embedding model and sets the default index to the token.

# Build vocab from glove_vectors
vocab = vocab(glove_embedding.stoi, 0, specials=('<unk>', '<pad>'))
vocab.set_default_index(vocab["<unk>"])
number of words in the vocab:

vocab_size = len(vocab)
vocab_size
400002
vocab(['he'])
[20]
Dataset splits
# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
 
num_train = int(len(train_dataset) * 0.05)
split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])
len(split_train_)
1250
len(split_valid_)
1250
len(test_dataset)
25000
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')
Data loader
The following code prepares the text processing pipeline with the tokenizer and vocabulary. The text pipeline is used to process the raw data strings from the dataset iterators.

The function text_pipeline first tokenizes the input text, then vocab is applied to get the token indices.

def text_pipeline(x):
    return vocab(tokenizer(x))
def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(_label)
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)
    return label_list.to(device), text_list.to(device)
BATCH_SIZE = 32

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
Let's check to see what these data loaders generate.

label, seqence=next(iter(valid_dataloader))
label, seqence
(tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
         1, 0, 0, 0, 0, 0, 1, 1]),
 tensor([[  39,   16,  967,  ...,    0,    0,    0],
         [  39,   16,    9,  ...,    0,    0,    0],
         [  43, 3318,   59,  ...,    0,    0,    0],
         ...,
         [4704, 9958,  321,  ...,    0,    0,    0],
         [  12,    2, 1001,  ...,    0,    0,    0],
         [2019,   25, 8966,  ...,   83, 1718,    4]]))
Neural Network
This code defines a class called Net that represents a text classifier based on a PyTorch TransformerEncoder. The constructor takes the following arguments:

num_class: The number of classes to classify
vocab_size: The size of the vocabulary
freeze: Whether to freeze the embedding layer
nhead: The number of heads in the transformer encoder
dim_feedforward: The dimension of the feedforward layer in the transformer encoder
num_layers: The number of transformer encoder layers
dropout: The dropout rate
activation: The activation function to use in the transformer encoder
classifier_dropout: The dropout rate for the classifier
Attributes:

emb: An embedding layer that maps each word in the vocabulary to a dense vector representation
pos_encoder: A positional encoding layer that adds positional information to the word vectors
transformer_encoder: A transformer encoder layer that processes the sequence of word vectors and extracts high-level features
classifier: A linear layer that maps the output of the transformer encoder to the desired number of classes
glove_embedding.vectors.shape
torch.Size([400000, 100])
Let's assume:

batch_size = 32 (we process 32 sentences at once)

seq_len = 50 (each sentence is padded/truncated to 50 tokens)

embedding_dim = 100 (from the pre-trained GloVe vectors)

num_class = 2 (e.g., positive/negative sentiment)


Input [32, 50]
↓
Embedding
↓
[32, 50, 100]
↓
Positional Encoding
↓
[32, 50, 100]
↓
Transformer Encoder
↓
[32, 50, 100]
↓
Mean Pooling (dim=1)
↓
[32, 100]
↓
Linear Classifier
↓
Output [32, 2]
class Net(nn.Module):
    """
    Text classifier based on a pytorch TransformerEncoder.
    """
    def __init__(
        self,
        num_class, vocab_size,
        freeze=True,
        nhead=2,
        dim_feedforward=128,
        num_layers=2,
        dropout=0.1,
        activation="relu",
        classifier_dropout=0.1):

        super().__init__()

        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors, freeze=freeze)
        embedding_dim = self.emb.embedding_dim


        self.pos_encoder = PositionalEncoding(
            d_model=embedding_dim,
            max_len=vocab_size,
            dropout=dropout
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
        )
        self.classifier = nn.Linear(embedding_dim, num_class)
        self.d_model = embedding_dim

    def forward(self, x):                           # Input Shape:  [batch_size, seq_len]
        x = self.emb(x) * math.sqrt(self.d_model)   # Output Shape: [batch_size, seq_len, embedding_dim]
        x = self.pos_encoder(x)                     # Output Shape: [batch_size, seq_len, embedding_dim]
        x = self.transformer_encoder(x)             # Output Shape: [batch_size, seq_len, embedding_dim]
        x = x.mean(dim=1)                           # Output Shape: [batch_size, embedding_dim]
        x = self.classifier(x)                      # Output Shape: [batch_size, num_class]

        return x
The model can then be trained on labeled data from the IMDB dataset with two classes.

Let's create the model.

model = Net(num_class=2, vocab_size=vocab_size).to(device)
model
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
The following predict function takes in a text, a text pipeline, and a model as inputs. It uses a pretrained model passed as a parameter to predict the label of the text for text classification on the IMDB dataset.

def predict(text, text_pipeline, model):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)), 0).to(device)
        model.to(device)
        output = model(text)
        return imdb_label[output.argmax(1).item()]
predict("I like sports and stuff", text_pipeline, model)
'negative review'
predict("I hate sports and stuff", text_pipeline, model)
'negative review'
 
def evaluate(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for label, text in tqdm(dataloader):
            label, text = label.to(device), text.to(device)
            output = model_eval(text)
            predicted = torch.max(output.data, 1)[1]
            total_acc += (predicted == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count
def evaluate_no_tqdm(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for label, text in dataloader:
            label, text = label.to(device), text.to(device)
            output = model_eval(text)
            predicted = torch.max(output.data, 1)[1]
            total_acc += (predicted == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count
The following code evaluates the performance of our model.

evaluate(valid_dataloader, model)
100%|██████████| 40/40 [00:10<00:00,  3.86it/s]
0.5184
Note that the current performance of the model is no better than average. This outcome is expected, considering that the model has not undergone any training yet.

Training
The following code defines the training function used to train model.

def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir="", file_name=None):
    cum_loss_list = []
    acc_epoch = []
    acc_old = 0
    model_path = os.path.join(save_dir, file_name)
    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_acc")
    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_loss")
    time_start = time.time()

    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        cum_loss = 0
        for idx, (label, text) in enumerate(train_dataloader):
            optimizer.zero_grad()
            label, text = label.to(device), text.to(device)

            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
        print(f"Epoch {epoch}/{epochs} - Loss: {cum_loss}")

        cum_loss_list.append(cum_loss)
        accu_val = evaluate_no_tqdm(valid_dataloader, model)
        acc_epoch.append(accu_val)

        if model_path and accu_val > acc_old:
            print(accu_val)
            acc_old = accu_val
            if save_dir is not None:
                print("save model epoch", epoch)
                torch.save(model.state_dict(), model_path)
                save_list_to_file(lst=acc_epoch, filename=acc_dir)
                save_list_to_file(lst=cum_loss_list, filename=loss_dir)
        print('='*50)

    time_end = time.time()
    print(f"Training time: {time_end - time_start}")
Train IMDB
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = "saved_model"
file_name = "model_IMDB dataset small2.pth"
train_model(model=model, 
            optimizer=optimizer, 
            criterion=criterion, 
            train_dataloader=train_dataloader, 
            valid_dataloader=valid_dataloader, 
            epochs=3, 
            save_dir=save_dir, 
            file_name=file_name
           )
  0%|          | 0/3 [00:00<?, ?it/s]
Epoch 1/3 - Loss: 29.780908584594727
0.5184
save model epoch 1
 33%|███▎      | 1/3 [00:38<01:17, 38.66s/it]
==================================================
Epoch 2/3 - Loss: 30.13214600086212
 67%|██████▋   | 2/3 [01:14<00:36, 37.00s/it]
==================================================
Epoch 3/3 - Loss: 29.726603388786316
100%|██████████| 3/3 [01:48<00:00, 36.18s/it]
==================================================
Training time: 108.55308437347412
This model was trained in a GPU environment.  
The following code plots the cost and validation data accuracy for each epoch of the pretrained model up to and including the epoch that yielded the highest accuracy

 
acc_epoch = load_list_from_file('acc_epoch')
cum_loss_list = load_list_from_file('cum_loss_list')
plot(cum_loss_list, acc_epoch)

 
As we can see, the pretrained model achieved an accuracy of over 85% on the validation set.

The following code loads pretrained model and evaluates its performance on the test set.

 
model_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_.load_state_dict(torch.load('my_imdb_model.pth', map_location=device))
<All keys matched successfully>
evaluate(test_dataloader, model_)
100%|██████████| 782/782 [02:43<00:00,  4.77it/s]
0.8298
As we can see, the pretrained model achieved an accuracy of approximately 83% on the test data.

 
 
 
Fine-tune a model pretrained on the AG News dataset
train_iter_ag_news = AG_NEWS(split="train")

num_class_ag_news_set = set([label for (label, text) in train_iter_ag_news])
print(num_class_ag_news_set)
num_class_ag_news = len(num_class_ag_news_set)
num_class_ag_news
{1, 2, 3, 4}
4
# Split the dataset into training and testing iterators.
train_iter_ag_news, test_iter_ag_news = AG_NEWS()
# Convert the training and testing iterators to map-style datasets.
train_dataset_ag_news = to_map_style_dataset(train_iter_ag_news)
test_dataset_ag_news = to_map_style_dataset(test_iter_ag_news)
# Determine the number of samples to be used for training and validation (5% for validation).
num_train_ag_news = int(len(train_dataset_ag_news) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_ag_news_, split_valid_ag_news_ = random_split(train_dataset_ag_news, [num_train_ag_news, len(train_dataset_ag_news) - num_train_ag_news])
num_train_ag_news = int(len(train_dataset_ag_news) * 0.05)
split_train_ag_news_, _ = random_split(split_train_ag_news_, [num_train_ag_news, len(split_train_ag_news_) - num_train_ag_news])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')
def label_pipeline(x):
   return int(x) - 1
def collate_batch_ag_news(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)
    
    return label_list.to(device), text_list.to(device)
BATCH_SIZE = 32

train_dataloader_ag_news = DataLoader(
    split_train_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
valid_dataloader_ag_news = DataLoader(
    split_valid_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
test_dataloader_ag_news = DataLoader(
    test_dataset_ag_news, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
model_ag_news = Net(num_class=4, vocab_size=vocab_size).to(device)
model_ag_news.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=4, bias=True)
)
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_ag_news.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_AG News small1.pth"
train_model(model=model_ag_news, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader_ag_news, valid_dataloader=valid_dataloader_ag_news,  epochs=2, save_dir=save_dir, file_name=file_name)
  0%|          | 0/2 [00:00<?, ?it/s]
Epoch 1/2 - Loss: 260.44899249076843
0.31383333333333335
save model epoch 1
 50%|█████     | 1/2 [00:18<00:18, 18.88s/it]
==================================================
Epoch 2/2 - Loss: 257.12995743751526
100%|██████████| 2/2 [00:36<00:00, 18.01s/it]
==================================================
Training time: 36.022834062576294
 
This model was trained on a GPU for 100 epochs
The following code plots the cost and validation data accuracy for each epoch of the pretrained model.

acc_epoch_ag = load_list_from_file('acc_epoch_ag')
cum_loss_list_ag = load_list_from_file('cum_loss_list_ag')
plot(cum_loss_list_ag, acc_epoch_ag)

the pretrained model achieved a very high accuracy of over 90% on the AG News validation set.

The following code loads the pretrained model and evaluates its performance on the AG News test set.

model_ag_news_ = Net(vocab_size=vocab_size, num_class=4).to(device)
model_ag_news_.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
evaluate(test_dataloader_ag_news, model_ag_news_)
100%|██████████| 238/238 [00:04<00:00, 51.68it/s]
0.9047368421052632
the pretrained model worked extremely well on the AG News dataset. However, can this model be fine-tuned to perform well on the IMDB dataset as well? Let's find out! we can start off by loading the pretrained AG News model.

model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine1.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
The IMDB dataset is a binary classification task with only two classes (positive and negative reviews). Therefore, the output layer of the AG NEWS model should be adjusted to have just two output neurons to reflect the binary nature of the IMDB dataset. This adjustment is essential for the model to accurately learn and predict the sentiment of movie reviews in the IMDB dataset.

model_fine1.classifier
in_features = model_fine1.classifier.in_features
print("Original final layer:", model_fine1.classifier)
print("Input dimention  final layer:", in_features)
Original final layer: Linear(in_features=100, out_features=4, bias=True)
Input dimention  final layer: 100
 
We can change the final layer into a two-class problem.

model_fine1.classifier = nn.Linear(in_features, 2)
model_fine1.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
model_fine1.classifier
in_features = model_fine1.classifier.in_features
print("Original final layer:", model_fine1.classifier)
print("Input dimention  final layer:", in_features)
Original final layer: Linear(in_features=100, out_features=2, bias=True)
Input dimention  final layer: 100
 
 
The following code shows the layers that are frozen (requires_grad == False) and unfrozen (requires_grad == True) in the model. The unfrozen layers will have their weights updated during fine-tuning.

for name, param in model_fine1.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: True
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: True
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: True
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: True
transformer_encoder.layers.0.linear1.weight requires_grad: True
transformer_encoder.layers.0.linear1.bias requires_grad: True
transformer_encoder.layers.0.linear2.weight requires_grad: True
transformer_encoder.layers.0.linear2.bias requires_grad: True
transformer_encoder.layers.0.norm1.weight requires_grad: True
transformer_encoder.layers.0.norm1.bias requires_grad: True
transformer_encoder.layers.0.norm2.weight requires_grad: True
transformer_encoder.layers.0.norm2.bias requires_grad: True
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: True
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: True
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: True
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: True
transformer_encoder.layers.1.linear1.weight requires_grad: True
transformer_encoder.layers.1.linear1.bias requires_grad: True
transformer_encoder.layers.1.linear2.weight requires_grad: True
transformer_encoder.layers.1.linear2.bias requires_grad: True
transformer_encoder.layers.1.norm1.weight requires_grad: True
transformer_encoder.layers.1.norm1.bias requires_grad: True
transformer_encoder.layers.1.norm2.weight requires_grad: True
transformer_encoder.layers.1.norm2.bias requires_grad: True
classifier.weight requires_grad: True
classifier.bias requires_grad: True
The following block simulates fine-tuning on the shortened training set for just 2 epochs.

LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine1.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine1.pth"
train_model(model=model_fine1, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )
  0%|          | 0/2 [00:00<?, ?it/s]
Epoch 1/2 - Loss: 27.95104557275772
0.4808
save model epoch 1
 50%|█████     | 1/2 [00:52<00:52, 52.32s/it]
==================================================
Epoch 2/2 - Loss: 27.739991307258606
0.5352
save model epoch 2
100%|██████████| 2/2 [01:49<00:00, 54.81s/it]
==================================================
Training time: 109.63206171989441
 
Using th model that fine-tuned on full IMDB training set for 100 epochs on a GPU
acc_epoch_imdb_ft = load_list_from_file('acc_epoch_imdb_ft')
cum_loss_list_imdb_ft = load_list_from_file('cum_loss_list_imdb_ft')
 
plot(cum_loss_list_imdb_ft, acc_epoch_imdb_ft)

 
 
The following line loads the prefine-tuned model and evaluates its performance on the IMDB test set.

model_fine1_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_fine1_.load_state_dict(torch.load('my_imdb_model_ft.pth', map_location=device))
evaluate(test_dataloader, model_fine1_)
100%|██████████| 782/782 [03:09<00:00,  4.12it/s]
0.85956
This model demonstrated notable improvement, exhibiting a remarkable achievement with an accuracy of 86% on the test data. This is higher than the 83% achieved by the model trained from scratch on the IMDB dataset.

 
 
Fine-tune the final layer only
Fine-tuning the final output layer of a neural network is similar to fine-tuning the whole model. we can begin by loading the pretrained model we would like to fine-tune. In this case, it is the same model pretrained on the AG News dataset.

model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine2.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
# Freeze all layers in the model
for param in model_fine2.parameters():
    param.requires_grad = False
dim = model_fine2.classifier.in_features
dim
100
model_fine2.classifier = nn.Linear(dim, 2)
model_fine2.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
for name, param in model_fine2.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.0.linear1.weight requires_grad: False
transformer_encoder.layers.0.linear1.bias requires_grad: False
transformer_encoder.layers.0.linear2.weight requires_grad: False
transformer_encoder.layers.0.linear2.bias requires_grad: False
transformer_encoder.layers.0.norm1.weight requires_grad: False
transformer_encoder.layers.0.norm1.bias requires_grad: False
transformer_encoder.layers.0.norm2.weight requires_grad: False
transformer_encoder.layers.0.norm2.bias requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.1.linear1.weight requires_grad: False
transformer_encoder.layers.1.linear1.bias requires_grad: False
transformer_encoder.layers.1.linear2.weight requires_grad: False
transformer_encoder.layers.1.linear2.bias requires_grad: False
transformer_encoder.layers.1.norm1.weight requires_grad: False
transformer_encoder.layers.1.norm1.bias requires_grad: False
transformer_encoder.layers.1.norm2.weight requires_grad: False
transformer_encoder.layers.1.norm2.bias requires_grad: False
classifier.weight requires_grad: True
classifier.bias requires_grad: True
 
 
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine2.pth"
train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )
  0%|          | 0/2 [00:00<?, ?it/s]
Epoch 1/2 - Loss: 27.813967764377594
0.548
save model epoch 1
 50%|█████     | 1/2 [00:22<00:22, 22.82s/it]
==================================================
Epoch 2/2 - Loss: 27.634103000164032
100%|██████████| 2/2 [00:44<00:00, 22.18s/it]
==================================================
Training time: 44.358015298843384
Loading a model fine-tuned on the full IMDB training set for 100 epochs.

acc_epoch_imdb_ft2 = load_list_from_file('acc_epoch_imdb_ft2')
cum_loss_list_imdb_ft2 = load_list_from_file('cum_loss_list_imdb_ft2')
plot(cum_loss_list_imdb_ft2, acc_epoch_imdb_ft2)

The following line loads the model fine-tuned for 100 epochs on the full IMDB train set and evaluates its performance on the IMDB test set.

model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_fine2_.load_state_dict(torch.load('model_fine2.pth', map_location=device))
evaluate(test_dataloader, model_fine2_)
100%|██████████| 782/782 [03:11<00:00,  4.07it/s]
0.63976
The previous code indicates that although fine-tuning the final layer takes a significantly smaller amount of time than fine-tuning the whole model, the performance of the model with just the last layer unfrozen is significantly worse (64% accuracy) than the fine-tuned model with all layers unfrozen (86% accuracy).

 
 
 
model_finetune_some_layers = Net(num_class=4, vocab_size=vocab_size).to(device)

# Freeze all layers in the model
for param in model_finetune_some_layers.parameters():
    param.requires_grad = False

model_finetune_some_layers
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=4, bias=True)
)
for name, param in model_finetune_some_layers.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.0.linear1.weight requires_grad: False
transformer_encoder.layers.0.linear1.bias requires_grad: False
transformer_encoder.layers.0.linear2.weight requires_grad: False
transformer_encoder.layers.0.linear2.bias requires_grad: False
transformer_encoder.layers.0.norm1.weight requires_grad: False
transformer_encoder.layers.0.norm1.bias requires_grad: False
transformer_encoder.layers.0.norm2.weight requires_grad: False
transformer_encoder.layers.0.norm2.bias requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.1.linear1.weight requires_grad: False
transformer_encoder.layers.1.linear1.bias requires_grad: False
transformer_encoder.layers.1.linear2.weight requires_grad: False
transformer_encoder.layers.1.linear2.bias requires_grad: False
transformer_encoder.layers.1.norm1.weight requires_grad: False
transformer_encoder.layers.1.norm1.bias requires_grad: False
transformer_encoder.layers.1.norm2.weight requires_grad: False
transformer_encoder.layers.1.norm2.bias requires_grad: False
classifier.weight requires_grad: False
classifier.bias requires_grad: False
# Unfreeze the "linear2" layers:
for i in range(2):
    for param in model_finetune_some_layers.transformer_encoder.layers[i].linear2.parameters():
        param.requires_grad = True

model_finetune_some_layers.classifier  = nn.Linear(100, 3)
 
for name, param in model_finetune_some_layers.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.0.linear1.weight requires_grad: False
transformer_encoder.layers.0.linear1.bias requires_grad: False
transformer_encoder.layers.0.linear2.weight requires_grad: True
transformer_encoder.layers.0.linear2.bias requires_grad: True
transformer_encoder.layers.0.norm1.weight requires_grad: False
transformer_encoder.layers.0.norm1.bias requires_grad: False
transformer_encoder.layers.0.norm2.weight requires_grad: False
transformer_encoder.layers.0.norm2.bias requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.1.linear1.weight requires_grad: False
transformer_encoder.layers.1.linear1.bias requires_grad: False
transformer_encoder.layers.1.linear2.weight requires_grad: True
transformer_encoder.layers.1.linear2.bias requires_grad: True
transformer_encoder.layers.1.norm1.weight requires_grad: False
transformer_encoder.layers.1.norm1.bias requires_grad: False
transformer_encoder.layers.1.norm2.weight requires_grad: False
transformer_encoder.layers.1.norm2.bias requires_grad: False
classifier.weight requires_grad: True
classifier.bias requires_grad: True
 






















The Hugging Face NLP Cookbook: Transformer Inference
Objective: This notebook is a hands-on guide to performing inference for a wide range of NLP tasks using the Hugging Face transformers library, contrasting the foundational manual approach with the powerful, high-level pipeline() abstraction.

Two Approaches to Inference
We explore the two primary ways to use pre-trained models, demonstrating the trade-offs between control and convenience.

Manual Inference (Under the Hood)	Pipeline Abstraction (Easy Mode)
Shows the foundational workflow: Tokenizer → Model → Logits → Post-Process.	Explores the high-level pipeline() function for rapid, one-line inference.
Provides a deeper understanding of the core mechanics of Transformers.	Ideal for quick prototyping and building production-ready applications.
NLP Task Showcase
This notebook provides practical, ready-to-run examples for several key NLP tasks:

Text Classification (Sentiment Analysis) with DistilBERT
Text Generation (Storytelling & Translation) with GPT-2 & T5
Fill-Mask with BERT
Language Detection with XLM-Roberta
This project serves as a practical reference for leveraging the power of pre-trained models, demonstrating both the fundamental mechanics and the efficient abstractions provided by the Hugging Face ecosystem.

# !pip install torch
# !pip install transformers
from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
Text classification with DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]
vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]
config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]
model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."
inputs = tokenizer(text, return_tensors="pt")
print(inputs)
{'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,
          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
Perform inference
with torch.no_grad():
    outputs = model(**inputs)
outputs
SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None)
with torch.no_grad():
    print(model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']))
SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None)
Get the logits
logits = outputs.logits
logits.shape
torch.Size([1, 2])
Post-process the output
Convert the logits to probabilities and get the predicted class:

# Convert logits to probabilities
probs = torch.softmax(logits, dim=-1)

# Get the predicted class
predicted_class = torch.argmax(probs, dim=-1)

# Map the predicted class to the label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class]
print(f"Predicted label: {predicted_label}")
Predicted label: POSITIVE
Text generation with GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]
vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]
config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]
generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")
inputs
{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
Perform inference
# Generate text
output_ids = model.generate(
    inputs.input_ids, 
    attention_mask=inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    max_length=50, 
    num_return_sequences=1
)

output_ids
tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,
         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,
         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,
          383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,
          373,  257]])
Post-process the output
# Decode the generated text
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(generated_text)
Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a
 
 
Hugging Face pipeline() function
Text classification using pipeline()
# Load a general text classification model
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
# Classify a sample text
result = classifier("Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.")
print(result)
Device set to use cpu
[{'label': 'POSITIVE', 'score': 0.9997586607933044}]
Language detection using pipeline()
classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
result = classifier("Bonjour, comment ça va?")
print(result)
config.json: 0.00B [00:00, ?B/s]
model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]
tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]
sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]
tokenizer.json: 0.00B [00:00, ?B/s]
special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]
Device set to use cpu
[{'label': 'fr', 'score': 0.9934879541397095}]
Text generation using pipeline()
# Initialize the text generation pipeline with GPT-2
generator = pipeline("text-generation", model="gpt2")
Device set to use cpu
# Generate text based on a given prompt
prompt = "Once upon a time"
result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)
print(result)
print(result[0]['generated_text'])
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
[{'generated_text': "Once upon a time, the universe had only one living thing: light. Its only life was light itself. When you were born, you didn't have to think about what life was like. You saw it in action. As you grew older, you began to see it in action. You saw the world through your eyes, and you began to see it through the eyes of your children.\n\nLife was light that the universe saw through your eyes. As you grew older, you began to see it in action. As you grew older, you began to see it through the eyes of your children. A light that I have seen through the eyes of my children, and through the eyes of my children. As I watch my children grow old and grow up and grow up. I see them with their eyes. I see them in action. I see them in the world through my eyes. I see them in a world they can only love. I see them in our faces without words. I see them through our eyes and through our minds.\n\nI see them through our eyes and through our minds. As I watch my children grow old and grow up and grow up, I see them in action. My children grow up and grow up. I see them in action through the eyes"}]
Once upon a time, the universe had only one living thing: light. Its only life was light itself. When you were born, you didn't have to think about what life was like. You saw it in action. As you grew older, you began to see it in action. You saw the world through your eyes, and you began to see it through the eyes of your children.

Life was light that the universe saw through your eyes. As you grew older, you began to see it in action. As you grew older, you began to see it through the eyes of your children. A light that I have seen through the eyes of my children, and through the eyes of my children. As I watch my children grow old and grow up and grow up. I see them with their eyes. I see them in action. I see them in the world through my eyes. I see them in a world they can only love. I see them in our faces without words. I see them through our eyes and through our minds.

I see them through our eyes and through our minds. As I watch my children grow old and grow up and grow up, I see them in action. My children grow up and grow up. I see them in action through the eyes
Text generation using T5 with pipeline()
# Initialize the text generation pipeline with T5
generator = pipeline("text2text-generation", model="t5-small")
config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]
model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]
generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]
tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]
spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]
tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]
Device set to use cpu
# Generate text based on a given prompt
prompt = "translate English to French: How are you?"
result = generator(prompt, max_length=50, num_return_sequences=1)
print(result[0]['generated_text'])
Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Comment êtes-vous?
Fill-mask task using BERT with pipeline()
# Initialize the fill-mask pipeline with BERT
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Generate text by filling in the masked token
prompt = "The capital of France is [MASK]."
result = fill_mask(prompt)

print(result)
config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]
model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]
vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]
tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]
Device set to use cpu
[{'score': 0.41678863763809204, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141655683517456, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.0633925348520279, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.0444474034011364, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.030297143384814262, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]
print(result[0]['sequence'])
the capital of france is paris.
 











Pre-training a Language Model from Scratch with Hugging Face
This notebook demystifies the foundational process of pre-training a language model. While most NLP tasks involve fine-tuning existing models, this project goes one level deeper to build a language model from its base components using the Hugging Face ecosystem. We will train a Transformer model on a large text corpus with a self-supervised objective.

The primary goal is to simulate the process that creates powerful models like BERT. We will accomplish this by implementing the Masked Language Modeling (MLM) task, where the model learns to predict randomly masked tokens in a sentence, thereby developing a deep contextual understanding of language.

Key Objectives
Master the Hugging Face Stack: Utilize datasets for efficient data handling, tokenizers to train a custom vocabulary, and transformers for model architecture and training.
Implement Pre-training: Set up and execute a Masked Language Modeling (MLM) pre-training run.
Build a Model from the Ground Up: Configure a Transformer model architecture (like RoBERTa or BERT) and train it on a custom dataset.
Understand Data Collation: Implement a data collator that dynamically creates training batches with masked tokens.
Workflow Overview
Dataset Preparation: Load a raw text corpus using the datasets library.
Tokenizer Training: Train a custom WordPiece or BPE tokenizer on the corpus to create a vocabulary tailored to the data.
Model Configuration: Initialize a standard Transformer architecture (e.g., BertForMaskedLM) with a custom configuration.
Data Collation for MLM: Use DataCollatorForLanguageModeling to prepare batches for training.
Training: Leverage the high-level Trainer API from Hugging Face to run the pre-training loop efficiently.
This project demonstrates a fundamental skill in modern AI: understanding how foundational models are created, not just how to use them.

# !pip install transformers==4.40.0 
# !pip install datasets 
# !pip install accelerate
# !pip install torch==2.3.0
import torch
from transformers import pipeline
from datasets import load_dataset
from transformers import AutoTokenizer, BertTokenizerFast, BertForMaskedLM, AutoModelForCausalLM, BertConfig, AutoModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer
import os
from tqdm.auto import tqdm

def warn(*args, **kwargs):
    pass

import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')



# from torch.optim.lr_scheduler import LambdaLR
# from torch.utils.data import DataLoader
# from torch.optim import AdamW
# from transformers import AutoConfig,AutoModelForCausalLM, AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,, TrainingArguments
# from transformers import AutoTokenizer, BertTokenizerFast, TextDataset, 


# import math
# import time
Disable tokenizer parallelism to avoid deadlocks.

import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
Pretraining and self-supervised fine-tuning
Pre-training Objectives
Masked Language Modeling (MLM)
Next Sentence Prediction (NSP)
Next Token Prediction
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
result = pipe("This movie was really")
result
[{'generated_text': 'This movie was really good. I was really surprised by how good it was.\nI was surprised'}]
print(result[0]["generated_text"])
This movie was really good. I was really surprised by how good it was.
I was surprised
 
Self-supervised training of a BERT model
Prepare the train dataset
Train a Tokenizer
Preprocess the dataset
Pre-train BERT using an MLM task
Evaluate the trained model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
print(dataset)
DatasetDict({
    test: Dataset({
        features: ['text'],
        num_rows: 4358
    })
    train: Dataset({
        features: ['text'],
        num_rows: 36718
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 3760
    })
})
dataset["train"][400]
{'text': " When Mason was injured in warm @-@ ups late in the year , Columbus was without an active goaltender on their roster . To remedy the situation , the team signed former University of Michigan goaltender Shawn Hunwick to a one @-@ day , amateur tryout contract . After being eliminated from the NCAA Tournament just days prior , Hunwick skipped an astronomy class and drove his worn down 2003 Ford Ranger to Columbus to make the game . He served as the back @-@ up to Allen York during the game , and the following day , he signed a contract for the remainder of the year . With Mason returning from injury , Hunwick was third on the team 's depth chart when an injury to York allowed Hunwick to remain as the back @-@ up for the final two games of the year . In the final game of the season , the Blue Jackets were leading the Islanders 7 – 3 with 2 : 33 remaining when , at the behest of his teammates , Head Coach Todd Richards put Hunwick in to finish the game . He did not face a shot . Hunwick was the franchise record ninth player to make his NHL debut during the season . Conversely , Vaclav Prospal played in his 1,000th NHL game during the year . \n"}
dataset["train"] = dataset["train"].select([i for i in range(1000)])
dataset["test"] = dataset["test"].select([i for i in range(200)])
dataset
DatasetDict({
    test: Dataset({
        features: ['text'],
        num_rows: 200
    })
    train: Dataset({
        features: ['text'],
        num_rows: 1000
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 3760
    })
})
output_file_train = "wikitext_dataset_train.txt"
output_file_test = "wikitext_dataset_test.txt"

with open(output_file_train, "w", encoding="utf-8") as f:
    for example in dataset["train"]:
        f.write(example["text"] + "\n")

with open(output_file_test, "w", encoding="utf-8") as f:
    for example in dataset["test"]:
        f.write(example["text"] + "\n")
dataset['train'][:4]['text']
['',
 ' = Valkyria Chronicles III = \n',
 '',
 ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the " Nameless " , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " Calamaty Raven " . \n']
create a tokenizer from existing one to re-use special tokens

bert_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
print(bert_tokenizer.special_tokens_map)
{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}
bert_tokenizer(['[UNK]'])
{'input_ids': [[101, 100, 102]], 'token_type_ids': [[0, 0, 0]], 'attention_mask': [[1, 1, 1]]}
bert_tokenizer.get_special_tokens_mask
<bound method PreTrainedTokenizerBase.get_special_tokens_mask of BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}>
print(bert_tokenizer.vocab_size)
30522
# model_name = 'bert-base-uncased'

# model = AutoModelForCausalLM.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)
Training a Tokenizer
def batch_iterator(batch_size=10000):
    for i in tqdm(range(0, len(dataset), batch_size)):
        yield dataset['train'][i: i + batch_size]["text"]

bert_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

## train the tokenizer using our own dataset
bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)
  0%|          | 0/1 [00:00<?, ?it/s]


len(bert_tokenizer.get_vocab())
12576
 
Pretraining
m = AutoModel.from_pretrained("bert-base-uncased")
print(m.config)
BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.40.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

# Define the BERT configuration
config = BertConfig(
    vocab_size=len(bert_tokenizer.get_vocab()),
    hidden_size=768,  
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072
)
Create the BERT model for pre-training:

model = BertForMaskedLM(config)
check model configuration

model
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(12576, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (transform_act_fn): GELUActivation()
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=768, out_features=12576, bias=True)
    )
  )
)
 
Tokenize Dataset Dynamically
dataset
DatasetDict({
    test: Dataset({
        features: ['text'],
        num_rows: 200
    })
    train: Dataset({
        features: ['text'],
        num_rows: 1000
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 3760
    })
})
def tokenize_function(examples):
    return bert_tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]
tokenized_datasets
DatasetDict({
    test: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 200
    })
    train: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1000
    })
    validation: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 3760
    })
})
print(tokenized_datasets["train"][0])
{'input_ids': [2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
train_dataset = tokenized_datasets["train"]
test_dataset = tokenized_datasets["test"]
print(train_dataset)
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1000
})
print(train_dataset[0])
{'input_ids': [2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Define the Data Collator for Language Modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)
Check how collator transforms a sample input data record

data_collator([train_dataset[0]])
{'input_ids': tensor([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])}
Now, we train the BERT Model using the Trainer module.

# Define the training arguments
training_args = TrainingArguments(
    output_dir="./trained_model",
    overwrite_output_dir=True,
    do_eval=True,
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    num_train_epochs=10, 
    per_device_train_batch_size=2,
    save_total_limit=2,  
    logging_steps=20
)

# Instantiate the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)
# Start the pre-training
trainer.train()
 
Evaluating Model Performance
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
 
Loading the saved model
This was trained in a GPU environment

model.resize_token_embeddings(30522)
model.load_state_dict(torch.load('bert-scratch-model.pt', map_location=torch.device('cpu')))
<All keys matched successfully>
# Define the input text with a masked token
text = "This is a [MASK] movie!"

# Create a pipeline for the "fill-mask" task
mask_filler = pipeline("fill-mask", model=model, tokenizer=bert_tokenizer)

# Generate predictions by filling the mask in the input text
results = mask_filler(text, top_k=5)
results
[{'score': 0.055178627371788025,
  'token': 18,
  'token_str': '/',
  'sequence': 'this is a / movie!'},
 {'score': 0.047895647585392,
  'token': 544,
  'token_str': '##ular',
  'sequence': 'this is aular movie!'},
 {'score': 0.04489510878920555,
  'token': 16,
  'token_str': '-',
  'sequence': 'this is a - movie!'},
 {'score': 0.0283290334045887,
  'token': 562,
  'token_str': '200',
  'sequence': 'this is a 200 movie!'},
 {'score': 0.024763470515608788,
  'token': 42,
  'token_str': 'h',
  'sequence': 'this is a h movie!'}]
for result in results:
    print(f"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}")
Predicted token: /, Confidence: 0.06
Predicted token: ##ular, Confidence: 0.05
Predicted token: -, Confidence: 0.04
Predicted token: 200, Confidence: 0.03
Predicted token: h, Confidence: 0.02
This weak performance can be due to insufficient training, lack of training data, model architecture, or not tuning hyperparameters. Let's try a pretrained model from Hugging Face:

Inferencing a pretrained BERT model
# Load the pretrained BERT model and tokenizer
pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

# Define the input text with a masked token
text = "This is a [MASK] movie!"

# Create the pipeline
mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)

# Perform inference using the pipeline
results = mask_filler(text)
for result in results:
    print(f"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}")
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Predicted token: great, Confidence: 0.16
Predicted token: horror, Confidence: 0.08
Predicted token: good, Confidence: 0.08
Predicted token: bad, Confidence: 0.05
Predicted token: fantastic, Confidence: 0.04
print(results[0]['sequence'])
this is a great movie!
This pretrianed model performs way better than the model we just trained for a few epochs using a single dataset.

 











Efficient Transformer Fine-Tuning: A Deep Dive into PEFT with Adapters
Objective: This notebook provides a comprehensive, hands-on comparison between traditional fine-tuning methods and the modern Parameter-Efficient Fine-Tuning (PEFT) technique using Adapters for text classification. The goal is to demonstrate how PEFT can achieve comparable performance to full fine-tuning while being significantly more computationally efficient.

Methodology & Core Concepts Covered
This project systematically explores four distinct strategies for training a Transformer-based text classifier:

Baseline Training: A Transformer Encoder model is trained from scratch on the IMDB Movie Reviews dataset.
Full Fine-Tuning (Transfer Learning): The model is pre-trained on the AG News dataset and then all of its parameters are fine-tuned on the downstream IMDB task.
Linear Probing (Head-Only Fine-Tuning): Only the final classification layer of the pre-trained model is trained, keeping all transformer layers frozen.
Parameter-Efficient Fine-Tuning (PEFT with Adapters): The pre-trained model's core weights are frozen, and lightweight "Adapter" modules are inserted and trained to adapt the model to the new task.
Key Findings & Conclusion
The experiments yield a clear and powerful conclusion:

Fine-Tuning Method	Test Accuracy (IMDB)	Efficiency Highlight
Trained from Scratch	~83%	High computational cost.
Full Fine-Tuning	~86%	Best performance, but trains all parameters.
Linear Probing	~64%	Very fast, but significant performance drop.
PEFT with Adapters	~86%	Matches full fine-tuning performance while training only a tiny fraction of the parameters.
This notebook conclusively demonstrates that PEFT with Adapters is a superior strategy for adapting large pre-trained models, offering the performance of full fine-tuning at a fraction of the computational cost.

Datasets: AG News, IMDB Movie Reviews
Core Libraries: PyTorch, TorchText
# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install portalocker==2.8.2
# !pip install torchdata==0.7.1
# !pip install numpy==1.26.0
# !pip install pandas
# !pip install matplotlib==3.9.0 scikit-learn==1.5.0
 
from tqdm import tqdm
import time
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import math

import torch
torch.set_num_threads(1)
from torch import nn
import os


from torch.utils.data import DataLoader

from torchtext.datasets import AG_NEWS
from IPython.display import Markdown as md

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors
from torchtext.datasets import IMDB
from torch.utils.data import Dataset
from torch.utils.data.dataset import random_split
from torchtext.vocab import GloVe,vocab
from torchtext.data.functional import to_map_style_dataset

import pickle

from urllib.request import urlopen
import io

import tarfile
import tempfile

from torch.nn.utils.rnn import pad_sequence

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
 
def plot(COST, ACC):

    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()

    plt.show()
def save_list_to_file(lst, filename):
    with open(filename, 'wb') as file:
        pickle.dump(lst, file)

def load_list_from_file(filename):
    with open(filename, 'rb') as file:
        loaded_list = pickle.load(file)
    return loaded_list
Positional encodings
class PositionalEncoding(nn.Module):
    """
    Implements the positional encoding layer for Transformer models.
    """
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create a matrix of shape (max_len, d_model) to hold all positional encodings.
        pe = torch.zeros(max_len, d_model)
        # Shape: [max_len, 1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # `div_term` calculates the denominator part of the formula: 1 / (10000^(2i/d_model)).
        # `2i` is represented by `torch.arange(0, d_model, 2)`.
        # Shape: [d_model / 2]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Broadcasting `position` [max_len, 1] with `div_term` [d_model/2]
        # results in a matrix of shape [max_len, d_model/2].
        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices
        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices

        # New shape: [1, max_len, d_model]
        pe = pe.unsqueeze(0)

        # `register_buffer` makes `pe` a part of the model's state, but not a parameter
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Adds positional encoding to the input tensor.

        Args:
            x (torch.Tensor): The input tensor of word embeddings.
                              Shape: [batch_size, seq_len, d_model]

        Returns:
            torch.Tensor: The input tensor with positional encodings added.
                          Shape: [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
PreTraining on IMDB data set
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')
tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))
tempdir = tempfile.TemporaryDirectory()
tar.extractall(tempdir.name)
tar.close()
tempdir.name
'/tmp/tmpy6vxhkl1'
os.listdir(tempdir.name)
['imdb_dataset']
 
class IMDBDataset(Dataset):
    def __init__(self, root_dir, train=True):
        """
        root_dir: The base directory of the IMDB dataset.
        train: A boolean flag indicating whether to use training or test data.
        """
        self.root_dir = os.path.join(root_dir, "train" if train else "test")
        self.neg_files = [os.path.join(self.root_dir, "neg", f) for f in os.listdir(os.path.join(self.root_dir, "neg")) if f.endswith('.txt')]
        self.pos_files = [os.path.join(self.root_dir, "pos", f) for f in os.listdir(os.path.join(self.root_dir, "pos")) if f.endswith('.txt')]
        self.files = self.neg_files + self.pos_files
        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)
        self.pos_inx = len(self.pos_files)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        return label, content
root_dir = tempdir.name + '/' + 'imdb_dataset'
train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data
test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data
start = train_iter.pos_inx
for i in range(-2, 2):
    print(train_iter[start+i])
    print('='*50)
(0, 'In a far away Galaxy is a planet called Ceta. It\'s native people worship cats. But the dog people wage war upon these feline loving people and they have no choice but to go to Earth and grind people up for food. This is one of the stupidest f#@k!ng ideas for a movie I\'ve seen. Leave it to Ted Mikels to make a movie more incompetent than the already low standard he set in previous films. It\'s like he enjoying playing in a celluloid game of Limbo. How low can he go? The only losers in the scenario are US the viewer. Mr. Mikels and his silly little handlebar mustache actually has people who STILL buy this crap.<br /><br />My Grade: F <br /><br />DVD Extras: Commentary by Ted Mikels; the Story behind the Making of (9 and a half minutes); 17 minutes, 15 seconds of Behind the scenes footage; Ted Mikels filmography; and Trailers for "The Worm Eaters" "Girl in Gold Boots", "the Doll Squad", "Ten Violent Women" (featuring nudity), "Blood Orgy of the She Devils", & "the Corpse Grinders"')
==================================================
(0, 'Adrianne, should really get a life-without Mr. "Brady". She nauseates me, and has been one of the main reasons why I know longer tune in to the show. It\'s pretty brainless show, and every little argument or disagreement seems to be put under the scope and analyzed to death. This makes them look/sound they are anything but ready for marriage, and yet, I know these disagreements are all part of life. I guess to some people this is entertainment. If this happens to fall into next season I will feel sorry for anyone who has nothing better to do with their life but watch this trash. Though I would not be terribly surprised. can\'t even stand the commercials for this show anymore! I hope they\'re getting enough money to constantly embarrass themselves in front of a camera week after week. However, the "A" girl has one heck of great butt!')
==================================================
(1, 'I was lucky enough to see this at a pre-screening last night (Oct. 20) and I was incredibly surprised by the wonderful plot and genuinely heart felt acting.<br /><br />While the plot is not particularly complicated or exceptionally new, the story unfolds in a way that feels fresh, unique, and distinctly "indy" in style. It isn\'t something that can easily be compared to films of the past, it\'s a unique take on a sort of classic middle-aged depressed love story.<br /><br />I was particularly struck by the casting of the film. Down to every last extra in the family, it was a beautiful and talented cast. The three daughters did a wonderful job, the talent was evenly dispersed between them and none of them "out-shone" the other two.<br /><br />It was truly a delightful film, appropriate for all ages and laugh out loud funny while also being truly touching and heart warming. It was a wonderful break from the sex jokes and nudity of recent films.')
==================================================
(1, 'Unfortunately, this movie never made it to DVD. I saw it when it was first released to the theaters in 1983, and then again when the VHS was released in 1992. When I recently saw a VHS copy at a flea market, I immediately bought it. I was not disappointed. First, the obvious: Claudia Ohana is beautiful and a joy to behold. But then, the film takes you into an unreal world where you have to reflect on your values and decide what is really important to you. The movie is about a lot of things. It is about how the World Bank and large corporations exploit and enslave developing countries with their capitalist schemes to force them into a debt that they can never repay. It is about how our economic system exploits us by forcing us into debt with credit cards, mortgage, and car payment. It is about trying to save an innocence that maybe we have never really had and maybe we cannot really save. It is about good and evil and about how hard it is sometimes to tell one from the other. It raises a lot of questions but does not give answers. I think sometimes this is good. For this reason, this is a film really worth saving and seeing.')
==================================================
 
imdb_label = {0: " negative review", 1: "positive review"}
imdb_label[1]
'positive review'
num_class = len(set([label for (label, text) in train_iter]))
num_class
2
tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)
glove_embedding = GloVe(name="6B", dim=100)
glove_embedding.vectors.shape
torch.Size([400000, 100])
Build vocab from glove_vectors

vocab = vocab(glove_embedding .stoi, min_freq=0, specials=('<unk>', '<pad>'))
vocab.set_default_index(vocab["<unk>"])
vocab_size = len(vocab)
vocab_size
400002
vocab(['he'])
[20]
Data set splits
# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
num_train = int(len(train_dataset) * 0.05)
split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])
print(len(split_train_))
print(len(split_valid_))
print(len(test_dataset))
1250
1250
25000
 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')
Data loader
def text_pipeline(x):
    return vocab(tokenizer(x))
def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(_label)
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)

    return label_list.to(device), text_list.to(device)
BATCH_SIZE = 32

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
label, seqence = next(iter(valid_dataloader))
label, seqence
(tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
         1, 1, 0, 1, 1, 0, 0, 1]),
 tensor([[ 12364,   3190,     17,  ...,      0,      0,      0],
         [    43,   3469,     43,  ...,      0,      0,      0],
         [    43,    915,     38,  ...,      4, 120433,      4],
         ...,
         [    51,   1679,  38445,  ...,      0,      0,      0],
         [    43,     17,  68175,  ...,      0,      0,      0],
         [     2,  40352,      3,  ...,      0,      0,      0]]))
label.shape
torch.Size([32])
seqence.shape
torch.Size([32, 1034])
Neural network
class Net(nn.Module):
    """
    Text classifier based on a pytorch TransformerEncoder.
    """
    def __init__(
        self,
        num_class, vocab_size,
        freeze=True,
        nhead=2,
        dim_feedforward=128,
        num_layers=2,
        dropout=0.1,
        activation="relu",
        classifier_dropout=0.1):

        super().__init__()

        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors, freeze=freeze)
        embedding_dim = self.emb.embedding_dim


        self.pos_encoder = PositionalEncoding(
            d_model=embedding_dim,
            dropout=dropout,
            max_len=vocab_size
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        self.classifier = nn.Linear(embedding_dim, num_class)
        self.d_model = embedding_dim

    def forward(self, x):
        x = self.emb(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)
        x = self.classifier(x)

        return x
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Net(num_class=2, vocab_size=vocab_size).to(device)
model
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
def predict(text, text_pipeline, model):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)), 0).to(device)
        model.to(device)
        output = model(text)
        return imdb_label[output.argmax(1).item()]
predict("I like sports and stuff", text_pipeline, model)
'positive review'
def evaluate(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for label, text in tqdm(dataloader):
            label, text = label.to(device), text.to(device)
            output = model_eval(text)
            predicted = torch.max(output.data, 1)[1]
            total_acc += (predicted == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count
def evaluate_no_tqdm(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for label, text in dataloader:
            label, text = label.to(device), text.to(device)
            output = model_eval(text)
            predicted = torch.max(output.data, 1)[1]
            total_acc += (predicted == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count
evaluate(valid_dataloader, model)
100%|██████████| 40/40 [00:13<00:00,  2.95it/s]
0.5096
current performance of the model is no better than average. This outcome is expected, considering that the model has not undergone any training yet.

Training
def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir="", file_name=None):
    cum_loss_list = []
    acc_epoch = []
    acc_old = 0
    model_path = os.path.join(save_dir, file_name)
    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_acc")
    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_loss")
    time_start = time.time()

    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        cum_loss = 0
        
        for idx, (label, text) in enumerate(train_dataloader):
            optimizer.zero_grad()
            label, text = label.to(device), text.to(device)

            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
        print(f"Epoch {epoch}/{epochs} - Loss: {cum_loss}")

        cum_loss_list.append(cum_loss)
        accu_val = evaluate_no_tqdm(valid_dataloader, model)
        acc_epoch.append(accu_val)

        if model_path and accu_val > acc_old:
            print("validation Acc:", accu_val)
            acc_old = accu_val
            if save_dir is not None:
                print("save model epoch", epoch)
                torch.save(model.state_dict(), model_path)
                save_list_to_file(lst=acc_epoch, filename=acc_dir)
                save_list_to_file(lst=cum_loss_list, filename=loss_dir)

    time_end = time.time()
    print(f"Training time: {time_end - time_start}")
Train IMDB
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_IMDB dataset small2.pth"
train_model(model=model, 
            optimizer=optimizer, 
            criterion=criterion, 
            train_dataloader=train_dataloader, 
            valid_dataloader=valid_dataloader, 
            epochs=2, 
            save_dir=save_dir, 
            file_name=file_name
           )
  0%|          | 0/2 [00:00<?, ?it/s]
Epoch 1/2 - Loss: 30.365230202674866
validation Acc: 0.5096
save model epoch 1
 50%|█████     | 1/2 [00:50<00:50, 50.36s/it]
Epoch 2/2 - Loss: 31.52835601568222
100%|██████████| 2/2 [01:37<00:00, 48.83s/it]
Training time: 97.65418004989624
 
Loading a model that has been pretrained using the same method but on the full data set and with 100 epochs on a GPU

acc_epoch = load_list_from_file('model_IMDB_acc')
cum_loss_list = load_list_from_file('model_IMDB_loss')
plot(cum_loss_list, acc_epoch)

the pretrained model achieved an accuracy of over 85% on the validation set.

model_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_.load_state_dict(torch.load('imdb_model_pretrained.pth', map_location=device))
<All keys matched successfully>
evaluate(test_dataloader, model_)
100%|██████████| 782/782 [03:20<00:00,  3.89it/s]
0.8308
The pretrained model achieved an accuracy of approximately 83% on the test data.

 
 
Fine-Tuning a model on IMDB Dataset that is pretrained on the AG News data set
Rather than training a model on the IMDB data set as we did earlier, we can fine-tune a model that has been pretrained on the AG News data set, which is a collection of news articles.

train_iter_ag_news = AG_NEWS(split="train")

num_class_ag_news_set = set([label for (label, text) in train_iter_ag_news])
print(num_class_ag_news_set)
num_class_ag_news = len(num_class_ag_news_set)
num_class_ag_news
{1, 2, 3, 4}
4
# Split the dataset into training and testing iterators.
train_iter_ag_news, test_iter_ag_news = AG_NEWS()
# Convert the training and testing iterators to map-style datasets.
train_dataset_ag_news = to_map_style_dataset(train_iter_ag_news)
test_dataset_ag_news = to_map_style_dataset(test_iter_ag_news)
# Determine the number of samples to be used for training and validation (5% for validation).
num_train_ag_news = int(len(train_dataset_ag_news) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_ag_news_, split_valid_ag_news_ = random_split(train_dataset_ag_news, [num_train_ag_news, len(train_dataset_ag_news) - num_train_ag_news])
num_train_ag_news = int(len(train_dataset_ag_news) * 0.05)
split_train_ag_news_, _ = random_split(split_train_ag_news_, [num_train_ag_news, len(split_train_ag_news_) - num_train_ag_news])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')
def label_pipeline(x):
    return int(x) - 1
def collate_batch_ag_news(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)
    
    return label_list.to(device), text_list.to(device)
BATCH_SIZE = 32

train_dataloader_ag_news = DataLoader(
    split_train_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
valid_dataloader_ag_news = DataLoader(
    split_valid_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
test_dataloader_ag_news = DataLoader(
    test_dataset_ag_news, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news
)
model_ag_news = Net(num_class=4, vocab_size=vocab_size).to(device)
model_ag_news.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=4, bias=True)
)
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_ag_news.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_AG News small1.pth"
train_model(model=model_ag_news, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader_ag_news, valid_dataloader=valid_dataloader_ag_news,  epochs=1, save_dir=save_dir, file_name=file_name)
  0%|          | 0/1 [00:00<?, ?it/s]
Epoch 1/1 - Loss: 267.9065834283829
validation Acc: 0.257
save model epoch 1
100%|██████████| 1/1 [00:19<00:00, 19.95s/it]
Training time: 19.95028018951416
loading a model that has been pretrained using the same method but on the full AG News data set for 100 epochs.

acc_epoch_ag = load_list_from_file('acc_epoch_ag')
cum_loss_list_ag = load_list_from_file('cum_loss_list_ag')
plot(cum_loss_list_ag, acc_epoch_ag)

the pretrained model achieved a very high accuracy of over 90% on the AG News validation set.

The following code loads the pretrained model and evaluates its performance on the AG News test set.

model_ag_news_ = Net(vocab_size=vocab_size, num_class=4).to(device)
model_ag_news_.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
evaluate(test_dataloader_ag_news, model_ag_news_)
100%|██████████| 238/238 [00:05<00:00, 42.24it/s]
0.9044736842105263
the pretrained model worked extremely well on the AG News dataset. However, can this model be fine-tuned to perform well on the IMDB dataset as well? Let's find out! we can start off by loading the pretrained AG News model.

 
 
Fine Tuning Ag News Model on IMDB dataset
model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine1.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
in_features = model_fine1.classifier.in_features
print("Original final layer:", model_fine1.classifier)
print("Input dimention  final layer:", in_features)
Original final layer: Linear(in_features=100, out_features=4, bias=True)
Input dimention  final layer: 100
model_fine1.classifier = nn.Linear(in_features, 2)
model_fine1.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
for name, param in model_fine1.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: True
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: True
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: True
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: True
transformer_encoder.layers.0.linear1.weight requires_grad: True
transformer_encoder.layers.0.linear1.bias requires_grad: True
transformer_encoder.layers.0.linear2.weight requires_grad: True
transformer_encoder.layers.0.linear2.bias requires_grad: True
transformer_encoder.layers.0.norm1.weight requires_grad: True
transformer_encoder.layers.0.norm1.bias requires_grad: True
transformer_encoder.layers.0.norm2.weight requires_grad: True
transformer_encoder.layers.0.norm2.bias requires_grad: True
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: True
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: True
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: True
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: True
transformer_encoder.layers.1.linear1.weight requires_grad: True
transformer_encoder.layers.1.linear1.bias requires_grad: True
transformer_encoder.layers.1.linear2.weight requires_grad: True
transformer_encoder.layers.1.linear2.bias requires_grad: True
transformer_encoder.layers.1.norm1.weight requires_grad: True
transformer_encoder.layers.1.norm1.bias requires_grad: True
transformer_encoder.layers.1.norm2.weight requires_grad: True
transformer_encoder.layers.1.norm2.bias requires_grad: True
classifier.weight requires_grad: True
classifier.bias requires_grad: True
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine1.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine1.pth"
train_model(model=model_fine1, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=1,  save_dir=save_dir ,file_name=file_name )
  0%|          | 0/1 [00:00<?, ?it/s]
Epoch 1/1 - Loss: 27.70047229528427
validation Acc: 0.516
save model epoch 1
100%|██████████| 1/1 [00:59<00:00, 59.12s/it]
Training time: 59.12255787849426
 
The following code shows the progress of full fine-tuning of the entire IMDB training set for 100 epochs.

acc_epoch_imdb_ft = load_list_from_file('acc_epoch_imdb_ft')
cum_loss_list_imdb_ft = load_list_from_file('cum_loss_list_imdb_ft')
plot(cum_loss_list_imdb_ft, acc_epoch_imdb_ft)

# The following line loads the prefine-tuned model and evaluates its performance on the IMDB test set.

model_fine1_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_fine1_.load_state_dict(torch.load('my_imdb_model_ft.pth', map_location=device))
evaluate(test_dataloader, model_fine1_)
100%|██████████| 782/782 [03:16<00:00,  3.99it/s]
0.85876
This model demonstrated notable improvement, exhibiting a remarkable achievement with an accuracy of 86% on the test data. This is higher than the 83% achieved by the model trained from scratch on the IMDB dataset.

 
Fine-tune the final layer only
model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine2.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
# Freeze all layers in the model
for param in model_fine2.parameters():
    param.requires_grad = False
dim = model_fine2.classifier.in_features
dim
100
model_fine2.classifier = nn.Linear(dim, 2)
model_fine2.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Linear(in_features=100, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=100, bias=True)
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
for name, param in model_fine2.named_parameters():
    print(f"{name} requires_grad: {param.requires_grad}")
emb.weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.0.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.0.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.0.linear1.weight requires_grad: False
transformer_encoder.layers.0.linear1.bias requires_grad: False
transformer_encoder.layers.0.linear2.weight requires_grad: False
transformer_encoder.layers.0.linear2.bias requires_grad: False
transformer_encoder.layers.0.norm1.weight requires_grad: False
transformer_encoder.layers.0.norm1.bias requires_grad: False
transformer_encoder.layers.0.norm2.weight requires_grad: False
transformer_encoder.layers.0.norm2.bias requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_weight requires_grad: False
transformer_encoder.layers.1.self_attn.in_proj_bias requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.weight requires_grad: False
transformer_encoder.layers.1.self_attn.out_proj.bias requires_grad: False
transformer_encoder.layers.1.linear1.weight requires_grad: False
transformer_encoder.layers.1.linear1.bias requires_grad: False
transformer_encoder.layers.1.linear2.weight requires_grad: False
transformer_encoder.layers.1.linear2.bias requires_grad: False
transformer_encoder.layers.1.norm1.weight requires_grad: False
transformer_encoder.layers.1.norm1.bias requires_grad: False
transformer_encoder.layers.1.norm2.weight requires_grad: False
transformer_encoder.layers.1.norm2.bias requires_grad: False
classifier.weight requires_grad: True
classifier.bias requires_grad: True
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine2.pth"
train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=1,  save_dir=save_dir ,file_name=file_name )
  0%|          | 0/1 [00:00<?, ?it/s]
Epoch 1/1 - Loss: 27.468307673931122
validation Acc: 0.4904
save model epoch 1
100%|██████████| 1/1 [00:26<00:00, 26.77s/it]
Training time: 26.768023014068604
acc_epoch_imdb_ft2 = load_list_from_file('acc_epoch_imdb_ft2')
cum_loss_list_imdb_ft2 = load_list_from_file('cum_loss_list_imdb_ft2')
plot(cum_loss_list_imdb_ft2, acc_epoch_imdb_ft2)

model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)
model_fine2_.load_state_dict(torch.load('model_fine2.pth', map_location=device))
evaluate(test_dataloader, model_fine2_)
100%|██████████| 782/782 [03:07<00:00,  4.16it/s]
0.63824
This indicates that although fine-tuning the final layer takes a significantly smaller amount of time than fine-tuning the whole model, the performance of the model with just the last layer unfrozen is significantly worse (64% accuracy) than the fine-tuned model with all layers unfrozen (86% accuracy).

 
 
Adapters
FeatureAdapter is a neural network module that introduces a low-dimensional bottleneck in a transformer architecture to allow fine-tuning with fewer parameters. It compresses the original high-dimensional embeddings into a lower dimension, applies a non-linear transformation, and then expands it back to the original dimension. This process is followed by a residual connection that adds the transformed output back to the original input to preserve information and promote gradient flow.

class FeatureAdapter(nn.Module):
    """
    A Parameter-Efficient Fine-Tuning (PEFT) adapter module.

    This module is designed to be inserted between layers of a larger pre-trained
    model. It uses a bottleneck architecture to learn task-specific adjustments
    while keeping the base model's weights frozen.
    """
    def __init__(self, model_dim: int, bottleneck_size: int = 64):
        """
        Initializes the FeatureAdapter.

        Args:
            model_dim (int): The feature dimension of the pre-trained model's layers.
            bottleneck_size (int): The smaller, intermediate dimension of the adapter.
        """
        super().__init__()
        
        self.bottleneck_transform = nn.Sequential(
            nn.Linear(model_dim, bottleneck_size),
            nn.ReLU(),
            nn.Linear(bottleneck_size, model_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Defines the forward pass of the adapter.

        It processes the input through the bottleneck and adds the result back
        to the original input using a residual connection.

        Args:
            x (torch.Tensor): Input tensor from the previous layer of the base model.
                              Shape: (batch_size, seq_length, model_dim).

        Returns:
            torch.Tensor: The output tensor with the adapter's adjustments.
                          Shape: (batch_size, seq_length, model_dim).
        """
        # Get the task-specific adjustment from the adapter
        adjustment = self.bottleneck_transform(x)
        
        # Add the adjustment to the original input (residual connection)
        output = x + adjustment
        
        return output
The adapted class wraps this adapter functionality around any specified linear layer, enhancing its output with the non-linearity of a ReLU activation function. This setup is particularly useful for experimenting with subtle architectural modifications in deep learning models, facilitating fine-tuning and potentially improving model performance on complex tasks.

class Adapted(nn.Module):
    """
    Wraps a pre-existing nn.Linear layer with a FeatureAdapter.

    This allows for easy, modular insertion of adapters into a pre-trained
    model's architecture for parameter-efficient fine-tuning.
    """
    def __init__(self, linear_layer, bottleneck_size=None):
        """
        Initializes the AdaptedLinear module.

        Args:
            linear_layer (nn.Linear): The original linear layer from the pre-trained model.
            bottleneck_size (Optional[int]): The intermediate dimension for the adapter.
                                             If None, defaults to half of the model's dimension.
        """
        super().__init__()
        
        # Keep the original, pre-trained linear layer
        self.linear = linear_layer
        
        # Automatically infer the model dimension from the original layer
        model_dim = self.linear.out_features
        
        # Set a sensible default for the bottleneck size if not provided
        if bottleneck_size is None:
            bottleneck_size = model_dim // 2
            
        # Initialize the adapter to process the output of the linear layer
        self.adaptor = FeatureAdapter(
            model_dim=model_dim,
            bottleneck_size=bottleneck_size
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Defines the forward pass.

        The input is first processed by the original linear layer and then
        immediately passed through the feature adapter.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor after passing through the linear layer
                          and its attached adapter.
        """
        # Pass input through the original frozen layer
        linear_output = self.linear(x)
        
        # Apply the trainable adapter to the output
        adapted_output = self.adaptor(linear_output)
        
        return adapted_output
we load the pretrained transformer model that was trained on the AG News dataset.

model_adapters = Net(vocab_size=vocab_size, num_class=4).to(device)
model_adapters.load_state_dict(torch.load('my_agnews_model.pth', map_location=device))
<All keys matched successfully>
First, freeze the parameters of a model named model_adapters to prevent them from being updated during training. Then, retrieve the number of input features for the classifier, and replace the classifier with a new linear layer that outputs to two classes.

for param in model_adapters.parameters():
    param.requires_grad = False

dim = model_adapters.classifier.in_features
model_adapters.classifier = nn.Linear(dim, 2)
model_adapters.transformer_encoder.layers[0]
TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
  )
  (linear1): Linear(in_features=100, out_features=128, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=128, out_features=100, bias=True)
  (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)
my_example_layer = model_adapters.transformer_encoder.layers[0].linear1
print(my_example_layer)
Linear(in_features=100, out_features=128, bias=True)
copy the linear layer and add an adapter layer to it.

my_adapeted_layer = Adapted(my_example_layer)
my_adapeted_layer
Adapted(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=128, bias=True)
    )
  )
)
for name, param in my_adapeted_layer.named_parameters():
    print(f"{name}: requires_grad={param.requires_grad}")
linear.weight: requires_grad=False
linear.bias: requires_grad=False
adaptor.bottleneck_transform.0.weight: requires_grad=True
adaptor.bottleneck_transform.0.bias: requires_grad=True
adaptor.bottleneck_transform.2.weight: requires_grad=True
adaptor.bottleneck_transform.2.bias: requires_grad=True
# Find number of layers
N_layers = len(model_adapters.transformer_encoder.layers)
N_layers
2
model_adapters.transformer_encoder.layers[0]
TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
  )
  (linear1): Linear(in_features=100, out_features=128, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=128, out_features=100, bias=True)
  (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)
model_adapters.transformer_encoder.layers[1]
TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
  )
  (linear1): Linear(in_features=100, out_features=128, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=128, out_features=100, bias=True)
  (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)
# Traverse model and adapt
for n in range(N_layers):
    encoder = model_adapters.transformer_encoder.layers[n]
    if encoder.linear1:
        print(" before linear1")
        print(encoder.linear1)
        model_adapters.transformer_encoder.layers[n].linear1=Adapted(encoder.linear1, bottleneck_size=24)
        print(" after  linear1")
        print(model_adapters.transformer_encoder.layers[n].linear1)

    if encoder.linear2:
        print(" before linear2")
        print(model_adapters.transformer_encoder.layers[n].linear2)
        model_adapters.transformer_encoder.layers[n].linear2=Adapted(encoder.linear2, bottleneck_size=24)
        print(" after linear2")
        print(model_adapters.transformer_encoder.layers[n].linear2)
 before linear1
Linear(in_features=100, out_features=128, bias=True)
 after  linear1
Adapted(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=128, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=128, bias=True)
    )
  )
)
 before linear2
Linear(in_features=128, out_features=100, bias=True)
 after linear2
Adapted(
  (linear): Linear(in_features=128, out_features=100, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=100, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=100, bias=True)
    )
  )
)
 before linear1
Linear(in_features=100, out_features=128, bias=True)
 after  linear1
Adapted(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=128, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=128, bias=True)
    )
  )
)
 before linear2
Linear(in_features=128, out_features=100, bias=True)
 after linear2
Adapted(
  (linear): Linear(in_features=128, out_features=100, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=100, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=100, bias=True)
    )
  )
)
model_adapters.to(device)
Net(
  (emb): Embedding(400000, 100)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
        )
        (linear1): Adapted(
          (linear): Linear(in_features=100, out_features=128, bias=True)
          (adaptor): FeatureAdapter(
            (bottleneck_transform): Sequential(
              (0): Linear(in_features=128, out_features=24, bias=True)
              (1): ReLU()
              (2): Linear(in_features=24, out_features=128, bias=True)
            )
          )
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Adapted(
          (linear): Linear(in_features=128, out_features=100, bias=True)
          (adaptor): FeatureAdapter(
            (bottleneck_transform): Sequential(
              (0): Linear(in_features=100, out_features=24, bias=True)
              (1): ReLU()
              (2): Linear(in_features=24, out_features=100, bias=True)
            )
          )
        )
        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
)
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_adapters.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_adapters.pth"
train_model(model=model_adapters, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )
  0%|          | 0/2 [00:00<?, ?it/s]
Epoch 1/2 - Loss: 27.904849410057068
validation Acc: 0.4872
save model epoch 1
 50%|█████     | 1/2 [00:42<00:42, 42.95s/it]
Epoch 2/2 - Loss: 27.942559123039246
validation Acc: 0.5104
save model epoch 2
100%|██████████| 2/2 [01:25<00:00, 42.63s/it]
Training time: 85.26155400276184
The training of an adapted model fine-tuned on the full IMDB dataset for 100 epochs on a GPU.

acc_epoch = load_list_from_file('adapter_model_acc')
cum_loss_list = load_list_from_file('adapter_model_loss')
plot(cum_loss_list, acc_epoch)

The following code loads the adapted model fine-tuned for 100 epochs on the full IMDB train set and evaluates its performance on the IMDB test set.

model_adapters_ = Net(vocab_size=vocab_size, num_class=2).to(device)
for n in range(N_layers):
    encoder = model_adapters_.transformer_encoder.layers[n]
    if encoder.linear1:
        print(" before linear1")
        print(encoder.linear1)
        model_adapters_.transformer_encoder.layers[n].linear1=Adapted(encoder.linear1, bottleneck_size=24)
        print(" after  linear1")
        print(model_adapters_.transformer_encoder.layers[n].linear1)

    if encoder.linear2:
        print(" before linear2")
        print(model_adapters_.transformer_encoder.layers[n].linear2)
        model_adapters_.transformer_encoder.layers[n].linear2=Adapted(encoder.linear2, bottleneck_size=24)
        print(" after linear2")
        print(model_adapters_.transformer_encoder.layers[n].linear2)

model_adapters_.to(device)
for param in model_adapters_.parameters():
    param.requires_grad = False

model_adapters_.load_state_dict(torch.load('model_adapters.pth', map_location=device))
 before linear1
Linear(in_features=100, out_features=128, bias=True)
 after  linear1
Adapted(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=128, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=128, bias=True)
    )
  )
)
 before linear2
Linear(in_features=128, out_features=100, bias=True)
 after linear2
Adapted(
  (linear): Linear(in_features=128, out_features=100, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=100, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=100, bias=True)
    )
  )
)
 before linear1
Linear(in_features=100, out_features=128, bias=True)
 after  linear1
Adapted(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=128, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=128, bias=True)
    )
  )
)
 before linear2
Linear(in_features=128, out_features=100, bias=True)
 after linear2
Adapted(
  (linear): Linear(in_features=128, out_features=100, bias=True)
  (adaptor): FeatureAdapter(
    (bottleneck_transform): Sequential(
      (0): Linear(in_features=100, out_features=24, bias=True)
      (1): ReLU()
      (2): Linear(in_features=24, out_features=100, bias=True)
    )
  )
)
<All keys matched successfully>
evaluate(test_dataloader, model_adapters_)
100%|██████████| 782/782 [04:12<00:00,  3.10it/s]
0.8564
 
As we can see, the performance of the fine-tuned adapted model is nearly identical to the fully fine-tuned model, with both models achieving a roughly 86% accuracy. This is an especially surprising result because a significantly smaller number of weights were updated for the adapted model than the fully fine-tuned model. Note that only the adapter layers with a bottleneck size of 24 and the final classifier layer are unfrozen.

The above shows that adapters can be used for parameter efficient fine-tuning (PEFT) and that the performance of a model fine-tuned using adapters can be almost as good as a fully fine-tuned model with all of the layers unfrozen!

 




















Mastering PEFT: A Deep Dive into LoRA from Scratch
Objective: This notebook provides a comprehensive exploration of Parameter-Efficient Fine-Tuning (PEFT) by implementing the Low-Rank Adaptation (LoRA) technique from scratch in PyTorch. We demonstrate its power and efficiency by fine-tuning a pre-trained text classifier and comparing its performance against a traditional training approach.

Methodology & Core Concepts
This project breaks down LoRA from theory to practical application in a few key steps:

Baseline Model: A standard text classification model is trained from scratch on the IMDB Movie Reviews dataset to establish a performance benchmark.
Theory of Low-Rank Adaptation: We visually and mathematically explore the concept of matrix rank, the core principle that makes LoRA efficient.
LoRA Layers from Scratch: We build the LoRALayer and LinearWithLoRA modules ourselves, showing exactly how the low-rank matrices (
 and 
) are created and integrated into a frozen linear layer.
PEFT in Action: We apply our custom LoRA layers to a model pre-trained on the AG News dataset, fine-tuning it for the IMDB sentiment analysis task by only training the lightweight LoRA weights.
Generalization: We demonstrate the versatility of LoRA by applying it to a completely different architecture—a Convolutional Neural Network (NNet).
Key Findings & Conclusion
The results clearly highlight the dual benefits of LoRA: improved performance and massive parameter efficiency.

Training Method	Test Accuracy (IMDB)	Parameter Efficiency
Trained from Scratch	~66.2%	Trains all parameters (~12,800 in the dense layer).
Fine-Tuned with LoRA	~69.2%	Trains only the LoRA weights (~450 parameters).
By fine-tuning with LoRA, we achieved a 3% absolute improvement in accuracy while training ~28 times fewer parameters in the adapted layer. This demonstrates that LoRA is a highly effective and efficient technique for adapting large pre-trained models to new tasks.

Datasets: AG News (for pre-training), IMDB Movie Reviews (for fine-tuning).
Core Libraries: PyTorch, TorchText.
# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install portalocker==2.8.2
# !pip install torchdata==0.7.1
# !pip install pandas
# !pip install matplotlib==3.9.0 scikit-learn==1.5.0
# !pip install numpy==1.26.0
 
import io
import math
import os
import pickle
import tarfile
import tempfile
import warnings
from itertools import accumulate
from urllib.request import urlopen

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split

import torchtext
from torchtext.data.functional import to_map_style_dataset
from torchtext.vocab import vocab
from torchtext.data.utils import get_tokenizer
from torch.nn.utils.rnn import pad_sequence
from torchtext.datasets import AG_NEWS
from torchtext.vocab import GloVe, Vectors, build_vocab_from_iterator


def warn(*args, **kwargs):
    """Custom warn function to suppress output."""
    pass

warnings.warn = warn
warnings.filterwarnings('ignore')
def plot(COST, ACC):
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()

    plt.show()
def save_list_to_file(lst, filename):
    with open(filename, 'wb') as file:
        pickle.dump(lst, file)

def load_list_from_file(filename):
    with open(filename, 'rb') as file:
        loaded_list = pickle.load(file)
    return loaded_list
Data pipeline
tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    for  _, text in data_iter:
        yield tokenizer(text)
glove_embedding = GloVe(name="6B", dim=100)
vocab = vocab(glove_embedding .stoi, min_freq=0, specials=('<unk>', '<pad>'))
vocab.set_default_index(vocab["<unk>"])
def text_pipeline(x):
  return vocab(tokenizer(x))

def label_pipeline(x):
   return int(x) 
IMDB dataset
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')
tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))
tempdir = tempfile.TemporaryDirectory()
tar.extractall(tempdir.name)
tar.close()
print(tempdir.name)
print(os.listdir(tempdir.name))
print(os.listdir(tempdir.name + '/' + 'imdb_dataset'))
print(os.listdir(tempdir.name + '/' + 'imdb_dataset/train'))
print(os.listdir(tempdir.name + '/' + 'imdb_dataset/train/pos')[:10])
/tmp/tmptqn2hay8
['imdb_dataset']
['test', 'train', '.DS_Store']
['pos', '.DS_Store', 'neg']
['9473_10.txt', '9890_8.txt', '542_9.txt', '7716_9.txt', '8582_9.txt', '4654_7.txt', '2706_9.txt', '8504_8.txt', '11710_10.txt', '2184_7.txt']
 
class IMDBDataset(Dataset):
    def __init__(self, root_dir, train=True):
        """
        root_dir: The base directory of the IMDB dataset.
        train: A boolean flag indicating whether to use training or test data.
        """
        self.root_dir = os.path.join(root_dir, "train" if train else "test")
        self.neg_files = [os.path.join(self.root_dir, "neg", f) for f in os.listdir(os.path.join(self.root_dir, "neg")) if f.endswith('.txt')]
        self.pos_files = [os.path.join(self.root_dir, "pos", f) for f in os.listdir(os.path.join(self.root_dir, "pos")) if f.endswith('.txt')]
        self.files = self.neg_files + self.pos_files
        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)
        self.pos_inx=len(self.pos_files)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
        return label, content
root_dir = tempdir.name + '/' + 'imdb_dataset'
train_iter = IMDBDataset(root_dir=root_dir, train=True)  
test_iter = IMDBDataset(root_dir=root_dir, train=False)  
start = train_iter.pos_inx
start = 0
for i in range(-2, 2):
    print(train_iter[start+i])
    print('=' * 100)
(1, "Have to be honest and say that I haven't seen many independent films, but I thought this one was very well done. The direction and cinematography were engaging without becoming a distraction. The angles, settings, and lighting used successfully created the nightmarish world in which the main character was trapped. Many haunting and memorable images from the film stick in your mind long after it's over (always a sign of a good director).")
====================================================================================================
(1, "What surprised me most about this film was the sheer audience it attracted. Similar films such as Anita and Me have never caused as much hype as this film has, though I think that's probably because of the mention of 'Beckham' in the title more than anything else.<br /><br />It's a brilliant film putting across a brilliant message - you can do anything if you're determined enough, and put your mind to it, which is such a positive message to anyone watching this film.<br /><br />I think this is one of Keira Knightley's better films, and I think she's a brilliant actress, and was excellent for the role. Parminder Nagra was brilliant too. Sadly, I can't say this for Jonathan Rhys-Meyers, because I don't think that he was that much of a good actor, and to be honest, his eyes were a little scary.<br /><br />All in all, a brilliant film, and a brilliant story")
====================================================================================================
(0, 'Five years after the original Creepshow, another inferior horror sequel is penned by George A. Romero and Stephen King: Creepshow 2. This time there are only three stories instead of five. None of the three stories is really original or distinguished either. The first story is a horror staple, formulaic story about a wooden Indian statue seeking revenge against the killers of its owners. The effects are really neat in this story, but it\'s just too familiar to be compelling enough. George Kennedy and Dorothy Lamour play the elderly store owners. The second story, "The Raft", is a Stephen King story. It\'s about four teenagers that unwittingly spend the day on a wooden pallet in the middle of an isolated lake. Soon the kids are screaming for their lives as a watery blob does each of them in for no apparent reason. However, instead of being suspenseful, the kids are saddled with bad dialog and dopey-headed behavior, preventing us from really caring about what happens next. There is also some unintentional humor in this segment. The third and final story is "The Hitch-hiker", which is actually a retread re-adapted for Creepshow 2. The original story, by Lucille Fletcher, was filmed in 1953 as a film noir suspense film. Then it was adapted for a famous Twilight Zone episode featuring Inger Stevens. "The Hitch-hiker" works the best out of these three offerings, but it\'s not without its problems either. Lois Chiles plays a cheating spouse, who ends up running over a hitch-hiker, or so she thinks. However, we don\'t know whether to sympathize with her or condemn her. As in many average stories of this type, the characters exist merely to tell the stories with their twists and turns. The wrap around story with the bullies seems a bit out of place. Tom Savini appears as the "creep" in this installment. The good thing is there haven\'t been any more sequels. *1/2 of 4 stars.')
====================================================================================================
(0, 'Is rich, ailing Elizabeth Taylor courting the Angel of Death on her island fortress in the Mediterranean, or is she just overreacting--or more precisely, overacting--as usual? Actually, both are applicable in director Joseph Losey\'s wandering, meandering mess called "Boom", appropriately titled since tempers in the lush, luxurious setting are nearly ready to explode. Richard Burton climbs Taylor\'s mountain uninvited; she dresses him in a samurai\'s robe complete with saber. Though great-looking in widescreen, the picture is otherwise quite deadly, a failure even Liz \'n Dick-philes should shun (the stars\' collective "what the hell!" attitude to their late-\'60s film careers reached an ego-mad nadir here). Pointless, confused, and maddening, "Boom" is a catastrophe--although screenwriter Tennessee Williams, who adapted his own unsuccessful play "The Milk Train Doesn\'t Stop Here Anymore", was said to be quite fond of it! * from ****')
====================================================================================================
imdb_label = {0: "negative review", 1: "positive review"}
imdb_label[1]
'positive review'
set([label for (label, text) in train_iter ])
{0, 1}
num_class = len(set([label for (label, text) in train_iter ]))
num_class
2
vocab(["age", "hello"])
[466, 13077]
Train and validate
# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
print(len(test_dataset))
print(len(split_train_))
print(len(split_valid_))
25000
23750
1250
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')
Data loader
def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)

    return label_list.to(device), text_list.to(device)
BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
label, seqence = next(iter(valid_dataloader ))
label, seqence
(tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
         1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
         1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1]),
 tensor([[    39,     16,      9,  ...,      0,      0,      0],
         [    51,      2, 270328,  ...,      0,      0,      0],
         [    43,   1908,     39,  ...,      0,      0,      0],
         ...,
         [   110,  15612,     35,  ...,      0,      0,      0],
         [   176,    533,      2,  ...,      5,     22,      4],
         [   203,    525,   1332,  ...,      0,      0,      0]]))
print(label.shape)
print(seqence.shape)
torch.Size([64])
torch.Size([64, 968])
 
Neural network
class TextClassifier(nn.Module):
    def __init__(self, num_classes, freeze=False):
        super(TextClassifier, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(glove_embedding.vectors.to(device), freeze=freeze)
        self.fc1 = nn.Linear(in_features=100, out_features=128)
        self.relu = nn.ReLU()
        # The output layer that gives the final probabilities for the classes
        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = torch.mean(x, dim=1)
        x = self.fc1(x)
        x = self.relu(x)
        return self.fc2(x)
Train the model on the full dataset
The model can then be trained on labeled data from the IMDB dataset with two classes.

model = TextClassifier(num_classes=2, freeze=True)
model.to(device)
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
model.eval()
predicted_label = model(seqence)
print(predicted_label.shape)
torch.Size([64, 2])
def predict(text, model, text_pipeline):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)), 0).to(device)
        output = model(text)
        return imdb_label[output.argmax(1).item()]
predict("the is a good movie", model, text_pipeline )
'negative review'
def evaluate(dataloader, model, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for label, text in dataloader:
            label, text = label.to(device), text.to(device)
            outputs = model(text)
            _, predicted = torch.max(outputs.data, 1)
            total += label.size(0)
            correct += (predicted == label).sum().item()
    accuracy = 100 * correct / total
    return accuracy
evaluate(test_dataloader, model, device)
50.0
performance of the model is no better than average. This outcome is expected, considering that the model has not undergone any training yet.

Train the model
def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name="model-imdb-freeze-true2"):
    cum_loss_list = []
    acc_epoch = []
    best_acc = 0
    file_name = model_name
    
    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        cum_loss = 0
        for _, (label, text) in enumerate(train_dataloader):            
            optimizer.zero_grad()
            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
            
        cum_loss_list.append(cum_loss)
        acc_val = evaluate(valid_dataloader, model, device)
        acc_epoch.append(acc_val)
        
        if acc_val > best_acc:
            best_acc = acc_val
            print(f"New best accuracy: {acc_val:.4f}")
            #torch.save(model.state_dict(), f"{model_name}.pth")
    
    #save_list_to_file(cum_loss_list, f"{model_name}-loss.pkl")
    #save_list_to_file(acc_epoch, f"{model_name}-acc.pkl")
LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
I have pretrained the model for 300 epochs using a GPU and saved this model.

# model_name = "model-imdb-freeze-true2"
# train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)
let's load the pretrained model that was trained for 300 epochs

model_name = "model-imdb-freeze-true2"

cum_loss_list = load_list_from_file(model_name + "-loss.pkl")
acc_epoch = load_list_from_file(model_name + "-acc.pkl")
plot(cum_loss_list, acc_epoch)

model.load_state_dict(torch.load(model_name + ".pth", map_location=device))
model.eval()
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
evaluate(test_dataloader , model, device)
66.24
The pretrained model achieves an accuracy of 66%.

 
 
Low-Rank Adaptation (LoRA)
We delve into building a LoRA (Low-Rank Adaptation) implementation from scratch using PyTorch. LoRA is a general method, but it's commonly applied to the Attention layer. For the sake of simplicity, we apply it to a Vanilla neural network.

LoRA

To train and predict, the forward pass holds $W_0$ constant.

Rank
The rank of a matrix is the number of dimensions the rows of the matrix "live in." A square matrix is said to be full rank if its rank is equal to the number of its rows or columns.

from sympy import Matrix, init_printing, Symbol
from numpy.linalg import qr, eig, inv, matrix_rank, norm
from scipy.linalg import null_space
init_printing()
def plot_matrix_and_subspace(F):
    assert F.shape[0] == 3, "Matrix F must have 3 rows to represent 3D space."

    ax = plt.figure().add_subplot(projection='3d')
    
    # Plot each column vector of F
    for i in range(F.shape[1]):
        ax.quiver(0, 0, 0, F[0, i], F[1, i], F[2, i], color='blue', arrow_length_ratio=0.1, label=f'Column {i+1}')

    # Calculate the null space of the transpose of F
    normal_vector = null_space(F.T)
    
    # Check that the null space is 1-dimensional
    if normal_vector.shape[1] == 1:
        normal_vector = normal_vector[:, 0]  # Simplify the array to 1D
        # Create a meshgrid for the plane
        xx, yy = np.meshgrid(np.linspace(-3, 3, 10), np.linspace(-3, 3, 10))
        # Calculate corresponding z coordinates based on the plane equation ax + by + cz = 0
        zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2] if normal_vector[2] != 0 else 0
        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Spanned Plane')
    else:
        print("The null space is not 1-dimensional, so a unique plane cannot be determined.")

    # Set plot limits and labels
    ax.set_xlim([-3, 3])
    ax.set_ylim([-3, 3])
    ax.set_zlim([-3, 3])
    ax.set_xlabel('X axis')
    ax.set_ylabel('Y axis')
    ax.set_zlabel('Z axis')
    ax.legend()

    plt.show()
B = torch.tensor([[1,0], [0,1], [0,0]]).numpy()
Matrix(B)
 
plot_matrix_and_subspace(B)

In this scenario, the vectors, despite each having three components, can reach any point on the two-dimensional green plane depicted in the image. These vectors span the green plane, which resides within a two-dimensional subspace. This subspace's dimension, also known as its 'rank', is two—corresponding to the dimensionality of the plane. If the rank were three, any point in the 3D space could be reached by some combination of the columns of 
. The rank of a matrix can be determined by using the matrix_rank function provided by NumPy.

matrix_rank(B)
2
Here, we plot a different matrix where the matrix spans a different plane, but the rank remains two.

B_= torch.tensor([[1,0], [-2,1], [0,1]]).numpy()
plot_matrix_and_subspace(B_)
print("rank of B", matrix_rank(B_))

rank of B 2
Here,we present the matrix A. The rank of this matrix is also two.

A = torch.tensor([[1, 1, -1, 1, 0], [-2, 2, 2, 0, 1]]).numpy()
Matrix(A)
 
matrix_rank(A)
2
For the matrices 
, if 
 and 
 both have a rank of 
:

C = B @ A
Matrix(C)
 
The columns of 
 will have the same rank as 
. Furthermore, the span of the columns of 
 will be the same as the span of the columns of 
.

print("rank of C", matrix_rank(C))
plot_matrix_and_subspace(C)
rank of C 2

LoRA in PyTorch
class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha


    # x @ self.A: The input x [batch_size, in_dim] (shape [in_dim, rank]), resulting in a tensor of shape [batch_size, rank].
    # @ self.B: resulting in a tensor of shape [batch_size, out_dim].
    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x
This class LinearWithLoRA copies the original linear model and creates a LoRALayer object.

Then, in the forward method apply both the original linear model and the output Lora model to the input x and add them together self.linear(x) + self.lora(x). This corresponds to:


class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear.to(device)
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        ).to(device)

    def forward(self, x):
        # W'x = W₀x + ΔWx
        return self.linear(x) + self.lora(x)
Applying LoRA
To fine-tune with LoRA, first, load a pretrained TextClassifier model with LoRA (while freezing its layers), load its pretrained state from a file, and then disable gradient updates for all of its parameters to prevent further training. Here, we will load a model that was pretrained on the AG NEWS dataset, which is a dataset that has 4 classes. we initialize this model, we set num_classes to 4. Moreover, the pretrained AG_News model was trained with the embedding layer unfrozen. Hence we will initialize the model with freeze=False. Although we are initializing the model with layers unfrozen and the wrong number of classes for our task, we will make modifications to the model later on that to correct this:

model_lora = TextClassifier(num_classes=4, freeze=False)
model_lora.to(device)
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=4, bias=True)
)
state_dict = torch.load("my-model-agnews-freeze-false.pth", map_location=device)
model_lora.load_state_dict(state_dict)
<All keys matched successfully>
# Here, we freeze all layers:
for parm in model_lora.parameters():
    parm.requires_grad=False
    
model_lora
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=4, bias=True)
)
Additionally, the original model was on a classification problem that had four classes, while the IMDB dataset has just 2 classes. To account for this, we will replace the final layer with a new linear layer where the number of outputs equals 2:

model_lora.fc2 = nn.Linear(in_features=128, out_features=2, bias=True).to(device)
model_lora
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
model_lora
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
now replace the hidden layer with a LoRA layer. we can access the hidden layer as

model_lora.fc1
Linear(in_features=100, out_features=128, bias=True)
The following replaces this layer with a LoRA layer:

model_lora.fc1 = LinearWithLoRA(model_lora.fc1, rank=2, alpha=0.1).to(device)
Let's look at the hidden layer again to ensure that it is indeed converted to a LoRA layer.

model_lora.fc1
LinearWithLoRA(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (lora): LoRALayer()
)
for name, param in model_lora.named_parameters():
    print(f"{name}: requires_grad={param.requires_grad}")
embedding.weight: requires_grad=False
fc1.linear.weight: requires_grad=False
fc1.linear.bias: requires_grad=False
fc1.lora.A: requires_grad=True
fc1.lora.B: requires_grad=True
fc2.weight: requires_grad=True
fc2.bias: requires_grad=True
 
At this point, training the model is similar, with the only difference being that, except for the output layer, only the learnable parameters A and B will be updated. The code to select the values for r and alpha, which is not run, is nonetheless provided herein for your convenience.

model_lora.to(device)
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): LinearWithLoRA(
    (linear): Linear(in_features=100, out_features=128, bias=True)
    (lora): LoRALayer()
  )
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
 
Let's set up the training components for the model_lora model

LR = 1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_lora.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
I have pretrained a model using an identical procedure for 300 epochs and saved it for reuse on a GPU.

# model_name = "model-lora-final2"
# train_model(model_lora,optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)
model_name = "model-lora-final2"
The following shows the progression of the training of this model for 300 epochs:

cum_loss_list=load_list_from_file(model_name+ "-loss.pkl")
acc_epoch=load_list_from_file(model_name + "-acc.pkl")
plot(cum_loss_list, acc_epoch)

Let's load the model into model_lora:

model_lora.load_state_dict(torch.load(model_name + ".pth", map_location=device))
model_lora.eval()
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): LinearWithLoRA(
    (linear): Linear(in_features=100, out_features=128, bias=True)
    (lora): LoRALayer()
  )
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
evaluate its performance on the test data.

evaluate(test_dataloader, model_lora, device)
we get a 3% improvement over a model trained from scratch by using LoRA. this occurs despite the fact that the model fine-tuned with LoRA updated less parameters than the model trained from scratch!

The model_lora.fc1 attribute represents LinearWithLoRA which contains both the standard Linear layer (linear) and an additional LoRA layer (lora) which represents the LoRALayer.

model_lora.fc1
LinearWithLoRA(
  (linear): Linear(in_features=100, out_features=128, bias=True)
  (lora): LoRALayer()
)
From model_lora.fc1.lora, we can obtain the learnable parameters A and B.

B = model_lora.fc1.lora.B
print("B", B)
print("\n Number of elements in the tensor B", B.numel())
torch.save(B, 'B.pth')
B Parameter containing:
tensor([[-4.3192e-01, -1.1071e+00,  2.4456e-01, -3.1031e-02, -2.0012e-01,
         -6.7811e-01, -1.3552e-01, -2.7458e-01,  3.2278e-02,  6.7592e-02,
          8.3020e-01,  1.1610e-05, -1.0894e-01,  7.7830e-05, -1.6789e-01,
         -1.3309e-01, -5.1875e-01,  2.1928e-02, -6.5869e-02, -3.5834e-01,
         -2.4473e-02, -1.1260e+00, -8.8752e-02, -7.0861e-03, -1.3263e-02,
          0.0000e+00, -6.9039e-01, -8.6471e-02, -3.9146e-01, -2.2644e-01,
         -8.7611e-01, -7.9929e-01,  0.0000e+00,  3.9646e-01,  5.2164e-01,
         -4.2730e-01,  2.3550e-01,  4.0447e-02,  2.3289e-01, -4.5217e-01,
          1.7721e-03, -4.7263e-01, -2.4343e-01,  6.3737e-01,  0.0000e+00,
          2.6904e-03, -7.8828e-01,  2.2559e-02, -4.3776e-02,  3.0909e-01,
         -1.6914e-01, -2.0294e-01, -4.2175e-01,  7.8840e-01, -3.1771e-01,
         -2.0639e-01,  1.1487e-02, -5.7238e-01,  1.4071e-01, -2.8561e-01,
          1.1753e-01, -1.6501e-04, -4.5406e-01,  0.0000e+00,  6.4464e-05,
         -4.4552e-01, -1.4372e-01,  5.2899e-02, -9.7813e-01, -4.5834e-01,
         -6.0968e-01,  5.5418e-02, -7.8054e-01, -1.2806e-01, -1.2291e-01,
         -1.4927e-01,  1.1984e-01, -4.9344e-02, -4.0802e-01,  0.0000e+00,
          1.0085e-01,  0.0000e+00, -1.1553e+00, -1.1726e-01,  0.0000e+00,
         -1.5980e-01,  1.0715e-01, -6.6047e-01, -4.0122e-01, -3.3328e-01,
          0.0000e+00, -8.4713e-05, -1.0361e+00, -2.0651e-01,  4.8110e-01,
         -1.1648e+00, -1.3535e+00,  3.9261e-01, -5.9634e-04,  0.0000e+00,
         -2.3484e-02, -2.9119e-02,  4.8014e-03, -9.4629e-02, -2.8612e-01,
         -2.1896e-02, -3.7316e-01,  3.3309e-02, -1.2363e-01, -1.7977e-01,
          4.3949e-05, -2.0580e-01,  4.9675e-01, -3.3358e-03,  2.3369e-01,
         -2.5254e-01,  8.1739e-03,  1.1209e-01, -1.7311e-01, -8.0624e-02,
         -1.3280e+00,  7.7398e-01, -1.2380e+00, -4.4729e-01, -6.5231e-01,
         -3.0026e-01,  8.8543e-02, -1.9287e-04],
        [ 2.8719e-01,  4.5585e-01, -2.3756e-01, -4.2161e-02,  1.0720e-01,
         -8.3839e-01,  2.5098e-01, -4.7875e-01, -6.7113e-02, -1.8940e-01,
         -9.7494e-01,  1.5431e-05, -1.9587e-01,  2.8327e-04,  1.3258e-01,
         -1.4904e-01,  7.6676e-01, -1.9980e-01, -5.4798e-02, -5.3408e-02,
         -1.8818e-01,  7.9588e-01,  9.6087e-02,  2.5530e-02, -7.2462e-01,
          0.0000e+00, -1.4258e+00, -5.9680e-02,  1.7253e-03,  4.7575e-02,
          4.2872e-01,  1.7875e-02,  0.0000e+00, -1.6925e-01,  1.6071e+00,
         -7.2869e-01, -2.5870e-01,  2.7839e-02, -3.0237e-01,  6.9624e-01,
         -2.8908e-04,  2.9164e-01,  4.7389e-02,  2.8530e-01,  0.0000e+00,
         -5.9235e-02, -6.8176e-02, -4.6494e-02, -8.1741e-02, -6.3252e-01,
          2.4469e-02,  7.2494e-02,  8.2158e-02,  6.3870e-01,  2.1799e-01,
          7.5177e-01,  3.1872e-02,  1.4100e-01, -2.3390e-01,  3.3391e-01,
         -3.8685e-01, -4.5463e-03,  1.4543e-01,  0.0000e+00,  3.8082e-05,
          3.0410e-01,  4.7614e-02, -1.5299e-01, -3.4549e-01, -2.3156e-01,
          2.2711e-01, -1.0894e-01,  1.7479e+00,  5.1043e-03,  1.4021e-02,
          1.9368e-02, -1.0875e-01,  9.7929e-03, -7.4530e-02,  0.0000e+00,
          1.9233e-02,  0.0000e+00,  5.1489e-01, -2.0680e-01,  0.0000e+00,
          1.8348e-01,  5.8013e-01,  2.9426e-01, -8.2005e-01,  2.2163e-01,
          0.0000e+00,  1.8530e-04,  5.1081e-01,  1.1597e-01, -4.4384e-01,
          6.3387e-01,  9.7338e-01, -6.0469e-01, -6.5377e-02,  0.0000e+00,
         -7.6416e-02, -4.1226e-02,  1.3478e-01, -2.0853e-01,  2.4558e-01,
         -1.0312e-01,  4.7792e-01,  4.1285e-02, -4.4301e-02,  3.2861e-02,
          6.9632e-04,  4.0895e-02,  1.1403e-01,  4.4740e-03, -2.8703e-01,
          3.8271e-01, -2.0000e-03, -5.1493e-02,  2.4292e-01, -2.3110e-01,
          1.1481e+00, -1.6331e+00,  5.0157e-01, -1.3528e-01,  3.8709e-01,
          1.4637e-01, -1.2716e-01, -6.3597e-05]], requires_grad=True)

 Number of elements in the tensor B 256
A = model_lora.fc1.lora.A
print("A", A)
print("\n Number of elements in the tensor A", A.numel())
torch.save(A, 'A.pth')
A Parameter containing:
tensor([[ 3.2952e-01,  9.3019e-01],
        [ 2.3904e+00, -5.1022e+00],
        [-2.4573e-01,  2.4733e+00],
        [ 5.8014e-01,  3.9014e-01],
        [ 1.6970e+00, -2.4614e+00],
        [-1.2420e+00,  8.3014e-01],
        [-2.0468e+00,  1.1629e+00],
        [-5.9361e-01,  1.8099e-01],
        [ 9.2466e-02,  1.1583e-01],
        [-2.0841e-02,  1.5550e+00],
        [ 1.3028e+00, -9.8381e-01],
        [ 1.4320e+00, -3.3497e+00],
        [-1.1637e+00,  1.9436e+00],
        [ 2.4898e-01, -1.1353e+00],
        [ 2.4423e+00,  1.0154e+00],
        [ 2.4881e+00, -4.6765e+00],
        [-1.8985e-01,  1.3426e+00],
        [-1.1730e-01, -2.1925e+00],
        [ 2.0193e+00, -8.5886e-01],
        [-3.1268e+00,  3.5134e+00],
        [ 1.0935e+00, -2.9263e+00],
        [-1.0435e+00,  2.5428e-01],
        [-8.6704e-01,  2.3570e+00],
        [-5.2725e-02, -2.3731e-01],
        [-4.3896e+00,  5.0369e+00],
        [-1.1174e+00,  9.9728e-01],
        [ 4.7710e-01, -1.2162e+00],
        [ 3.3296e+00, -5.9627e+00],
        [ 1.7393e+00, -2.0707e+00],
        [-4.0115e-01,  2.2691e+00],
        [ 4.8368e-04,  1.1773e+00],
        [ 1.9858e+00, -1.6100e+00],
        [ 6.9951e-01, -1.2869e+00],
        [-2.6380e-01,  2.5743e+00],
        [-3.9002e+00,  2.6775e+00],
        [-2.1946e+00,  3.7126e+00],
        [-3.2268e-01,  1.7346e-01],
        [ 4.7590e+00, -2.9553e+00],
        [-6.6704e-01,  8.1369e-01],
        [ 1.0073e+00,  3.6606e-01],
        [ 1.3186e-01, -3.4151e+00],
        [-6.6832e-01,  3.7761e-01],
        [-3.0825e-01,  1.1928e+00],
        [ 1.8897e+00, -1.2788e+00],
        [-1.4344e+00, -2.9294e-01],
        [ 2.1678e+00, -2.5469e+00],
        [-6.6993e-01,  2.2932e+00],
        [-3.1326e+00,  4.3424e+00],
        [ 4.2197e+00, -7.1153e+00],
        [ 7.1775e-01, -4.0832e-01],
        [-1.6389e+00,  1.4520e+00],
        [-1.6150e+00,  2.4683e+00],
        [ 1.5745e+00, -4.6085e+00],
        [ 6.0900e-01, -1.0851e+00],
        [-1.2270e+00,  4.0171e-01],
        [-2.6879e-01,  1.5706e+00],
        [ 2.0390e+00, -1.8152e+00],
        [-1.2317e+00,  6.0501e-01],
        [ 4.1927e-02, -2.4078e+00],
        [ 1.4265e+00, -9.2525e-01],
        [ 5.9394e-01, -7.4587e-01],
        [ 6.1463e-01, -2.6220e-01],
        [ 6.7605e-01, -2.1740e-01],
        [-5.9436e-01,  9.3632e-01],
        [ 2.8933e-01, -1.3208e-02],
        [-2.0206e+00,  3.9333e+00],
        [ 2.4903e+00, -1.1771e+00],
        [ 9.5112e-01, -1.7163e+00],
        [ 1.7097e+00, -8.8835e-01],
        [-3.4182e+00,  2.9024e+00],
        [ 1.3908e+00, -1.7813e+00],
        [-1.9825e+00,  2.1889e+00],
        [ 3.2187e-02, -1.7187e-01],
        [ 2.3456e+00, -3.3756e+00],
        [-1.3550e+00,  3.8261e+00],
        [-2.2111e+00,  1.7149e+00],
        [-1.6745e+00,  2.1577e+00],
        [-2.0270e+00,  3.8298e+00],
        [ 1.6872e+00, -1.8343e+00],
        [-1.3611e+00,  8.4912e-01],
        [-1.5436e+00,  2.8195e+00],
        [-8.4964e-01,  1.7983e+00],
        [ 1.7100e+00, -6.1945e-01],
        [ 1.7025e+00, -3.0842e+00],
        [ 1.5122e+00,  7.2826e-01],
        [ 9.1957e-01, -2.4107e+00],
        [-2.5545e+00,  4.1697e+00],
        [ 1.7832e-01, -2.9886e+00],
        [-4.0698e+00,  5.4074e+00],
        [ 1.3872e+00, -2.1134e+00],
        [-3.7505e+00,  4.7124e+00],
        [-1.6772e+00,  3.0610e+00],
        [-7.6127e-01,  7.7868e-01],
        [-5.1529e-01,  7.3124e-02],
        [ 2.5028e+00, -5.3530e-01],
        [ 2.1893e+00, -2.8006e+00],
        [-1.7899e+00,  1.7446e+00],
        [-1.1793e+00,  4.6499e+00],
        [ 1.1547e+00,  1.7930e+00],
        [-1.7421e-01,  2.7170e+00]], requires_grad=True)

 Number of elements in the tensor A 200
A and B have approximately 450 parameters. If we were to store the entire linear layer, we would have 12,800 parameters, which is around 28 times more.

print("\n Number of elements in the tensor A", model_lora.fc1.linear.weight.numel())
 Number of elements in the tensor A 12800
alfa and the ouput layer are also saved.

alfa_= model_lora.fc1.lora.alpha
torch.save(alfa_, 'alfa_.pth')
torch.save(model_lora.fc2.state_dict(), 'out_layer.pth')
Loading the model
The main advantage of LoRA is that for fine-tuning, we only need to save the learnable parameters A and B, alpha, and the output layer in our classification example.

The saved files are converted to tensors and the linear layer, respectively.

A = torch.load('A.pth')
print("A:", A.shape)
A: torch.Size([100, 2])
B = torch.load('B.pth')
print("B:", B.shape)
B: torch.Size([2, 128])
alfa_ = torch.load('alfa_.pth')
alfa_ 
The output layer:

output_layer = nn.Linear(in_features=128, out_features=2, bias=True)
output_layer.load_state_dict(torch.load('out_layer.pth'))
<All keys matched successfully>
The model object is created and the pretrained parameters are loaded:

model_load_lora = TextClassifier(num_classes=4, freeze=False)
model_load_lora.to(device)

state_dict = torch.load('my-model-agnews-freeze-false.pth', map_location=device)
model_load_lora.load_state_dict(state_dict)
model_load_lora
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): Linear(in_features=100, out_features=128, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=4, bias=True)
)
The LoRA layer object is added to the original hidden layer.

model_load_lora.fc1 = LinearWithLoRA(model_load_lora.fc1, rank=2, alpha=0.1)
model_load_lora.fc2 = nn.Linear(in_features=128, out_features=2, bias=True).to(device)
The parameters from fine-tuning are added.

model_load_lora.fc1.lora.A = A
model_load_lora.fc1.lora.B = B
model_load_lora.fc1.lora.alpha = alfa_ 
model_load_lora.fc2 = output_layer
model_load_lora.to(device)
model_load_lora.eval()
TextClassifier(
  (embedding): Embedding(400000, 100)
  (fc1): LinearWithLoRA(
    (linear): Linear(in_features=100, out_features=128, bias=True)
    (lora): LoRALayer()
  )
  (relu): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
)
evaluate(test_dataloader, model_load_lora, device)
This confirms that the model loaded correctly. we still get a 3% improvement in accuracy!

Finally, the following shows how we can make a prediction on the following article using the function predict:

article = """This was a lacklustre movie with very little going for it. I was not impressed."""
result = predict(article, model_load_lora, text_pipeline)
result
'negative review'
Applying LoRA to a different network
The following code defines a neural network called NNet.

NNet is a neural network that was originally written to identify hand-written digits from 32x32 images. our task is to fine-tune this network to perform letter recognition using LoRA. apply LoRA to just the second linear layer, and replace the last layer with a layer that has 26 outputs, one for each letter in the English alphabet.

class NNet(nn.Module):
    def __init__(self):
        super(NNet, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, input):
        # Convolution layer C1: 1 input image channel, 6 output channels,
        # 5x5 square convolution, it uses RELU activation function, and
        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch
        c1 = F.relu(self.conv1(input))
        # Subsampling layer S2: 2x2 grid, purely functional,
        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor
        s2 = F.max_pool2d(c1, (2, 2))
        # Convolution layer C3: 6 input channels, 16 output channels,
        # 5x5 square convolution, it uses RELU activation function, and
        # outputs a (N, 16, 10, 10) Tensor
        c3 = F.relu(self.conv2(s2))
        # Subsampling layer S4: 2x2 grid, purely functional,
        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor
        s4 = F.max_pool2d(c3, 2)
        # Flatten operation: purely functional, outputs a (N, 400) Tensor
        s4 = torch.flatten(s4, 1)
        # Fully connected layer F5: (N, 400) Tensor input,
        # and outputs a (N, 120) Tensor, it uses RELU activation function
        f5 = F.relu(self.fc1(s4))
        # Fully connected layer F6: (N, 120) Tensor input,
        # and outputs a (N, 84) Tensor, it uses RELU activation function
        f6 = F.relu(self.fc2(f5))
        # Gaussian layer OUTPUT: (N, 84) Tensor input, and
        # outputs a (N, 10) Tensor
        output = self.fc3(f6)
        return output

model = NNet()
model.to(device)

print('This is what the model looked like before applying LoRA:')
print(model)
print("\n###############\n")

# Freeze all parameters:
for parm in model.parameters():
    parm.requires_grad = False

# Change final layer for one with 26 outputs:
model.fc3 = nn.Linear(in_features=84, out_features=26, bias=True).to(device)

# Apply LoRA to the second linear layer
model.fc2=LinearWithLoRA(model.fc2, rank=2, alpha=0.1).to(device)

print('This is what the model looked like after applying LoRA:')
print(model)
This is what the model looked like before applying LoRA:
NNet(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)

###############

This is what the model looked like after applying LoRA:
NNet(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): LinearWithLoRA(
    (linear): Linear(in_features=120, out_features=84, bias=True)
    (lora): LoRALayer()
  )
  (fc3): Linear(in_features=84, out_features=26, bias=True)
)
 
 
 













The Applied Transformer: Fine-Tuning for Classification & Conversation
Objective: This notebook provides a practical guide to fine-tuning Transformer models for two distinct and critical NLP tasks: discriminative classification with an encoder (BERT) and generative instruction-following with a decoder (OPT).

Two Fine-Tuning Paths Explored
We demonstrate the versatility of pre-trained models by adapting them for fundamentally different goals using both foundational and high-level frameworks.

Path 1: Sentiment Classification	Path 2: Conversational AI
Model	bert-base-cased (Encoder)	facebook/opt-350m (Decoder)
Task	Multi-class classification of Yelp reviews (1-5 stars).	Instruction-tuning for a helpful assistant role.
Dataset	yelp_review_full	timdettmers/openassistant-guanaco
Framework	Native PyTorch training loop	Hugging Face SFTTrainer from the TRL library
Skills and Technologies
Models: BERT (Encoder Architecture), OPT (Decoder/Causal LM Architecture)
Frameworks: PyTorch, Hugging Face transformers, datasets, and trl
NLP Tasks: Supervised Fine-Tuning (SFT), Multi-Class Text Classification, Causal Language Modeling
Techniques: Custom PyTorch Training Loop, High-Level SFTTrainer, Data Collators for Instruction Tuning (DataCollatorForCompletionOnlyLM)
This project showcases an end-to-end understanding of how to adapt modern language models for both analytical and creative tasks, a core competency in applied AI/ML.

# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install portalocker==2.8.2
# !pip install torchdata==0.7.1
# !pip install pandas
# !pip install matplotlib==3.9.0 scikit-learn==1.5.0
# !pip install numpy==1.26.0
# !pip install transformers==4.42.1
# !pip install datasets
# !pip install torchmetrics==1.4.0.post0
# !pip install accelerate==0.31.0
# !pip install trl==0.9.4
# !pip install protobuf==3.20.*
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR

from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline
)

from transformers import (
    BertConfig,
    BertForMaskedLM,
    DataCollatorForLanguageModeling
)

from datasets import load_dataset
from trl import (
    SFTConfig,                 
    SFTTrainer,                
    DataCollatorForCompletionOnlyLM
)
from torchmetrics import Accuracy

from tqdm.auto import tqdm
import math
import time
import matplotlib.pyplot as plt

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
Supervised Fine-tuning with Pytorch
Dataset preparations
dataset = load_dataset("yelp_review_full")
dataset
DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 650000
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 50000
    })
})
dataset["train"][100]['text']
'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'
dataset["train"][100]["label"]
0
dataset["train"] = dataset["train"].select([i for i in range(1000)])
dataset["test"] = dataset["test"].select([i for i in range(200)])
dataset
DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 200
    })
})
Tokenizing data
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
print(tokenizer.model_max_length)
512
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]
tokenized_datasets
DatasetDict({
    train: Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 200
    })
})
tokenized_datasets['train'][0].keys()
dict_keys(['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'])
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets
DatasetDict({
    train: Dataset({
        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 200
    })
})
sample = tokenized_datasets["train"][0]
tokens = tokenizer.convert_ids_to_tokens(sample["input_ids"])

print(f"--- Sample 0 ---")
print(f"Label: {sample['labels']}")
print("-" * 20)

print(f"{'Token':<15} {'Input ID':<10} {'Attention Mask'}")
print("=" * 45)

for token, input_id, attention_mask in zip(tokens, sample["input_ids"], sample["attention_mask"]):
    if token == '[PAD]':
        print("\n... (padding tokens omitted)")
        break
    print(f"{token:<15} {input_id:<10} {attention_mask}")
--- Sample 0 ---
Label: 4
--------------------
Token           Input ID   Attention Mask
=============================================
[CLS]           101        1
d               173        1
##r             1197       1
.               119        1
gold            2284       1
##berg          2953       1
offers          3272       1
everything      1917       1
i               178        1
look            1440       1
for             1111       1
in              1107       1
a               170        1
general         1704       1
practitioner    22351      1
.               119        1
he              1119       1
'               112        1
s               188        1
nice            3505       1
and             1105       1
easy            3123       1
to              1106       1
talk            2037       1
to              1106       1
without         1443       1
being           1217       1
patron          10063      1
##izing         4404       1
;               132        1
he              1119       1
'               112        1
s               188        1
always          1579       1
on              1113       1
time            1159       1
in              1107       1
seeing          3195       1
his             1117       1
patients        4420       1
;               132        1
he              1119       1
'               112        1
s               188        1
affiliated      6559       1
with            1114       1
a               170        1
top             1499       1
-               118        1
notch           23555      1
hospital        2704       1
(               113        1
n               183        1
##yu            9379       1
)               114        1
which           1134       1
my              1139       1
parents         2153       1
have            1138       1
explained       3716       1
to              1106       1
me              1143       1
is              1110       1
very            1304       1
important       1696       1
in              1107       1
case            1692       1
something       1380       1
happens         5940       1
and             1105       1
you             1128       1
need            1444       1
surgery         6059       1
;               132        1
and             1105       1
you             1128       1
can             1169       1
get             1243       1
refer           5991       1
##rals          16179      1
to              1106       1
see             1267       1
specialists     18137      1
without         1443       1
having          1515       1
to              1106       1
see             1267       1
him             1140       1
first           1148       1
.               119        1
really          1541       1
,               117        1
what            1184       1
more            1167       1
do              1202       1
you             1128       1
need            1444       1
?               136        1
i               178        1
'               112        1
m               182        1
sitting         2807       1
here            1303       1
trying          1774       1
to              1106       1
think           1341       1
of              1104       1
any             1251       1
complaints      11344      1
i               178        1
have            1138       1
about           1164       1
him             1140       1
,               117        1
but             1133       1
i               178        1
'               112        1
m               182        1
really          1541       1
drawing         4619       1
a               170        1
blank           9153       1
.               119        1
[SEP]           102        1

... (padding tokens omitted)
 
DataLoader
tokenized_datasets["train"]
Dataset({
    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1000
})
train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=2)
eval_dataloader = DataLoader(tokenized_datasets["test"], batch_size=2)
Train the model
Load a pretrained model
Here, we'll load a pretrained classification model with 5 classes:

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Optimizer and learning rate schedule
optimizer = AdamW(model.parameters(), lr=5e-4)
num_epochs = 10
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda current_step: (1 - current_step / num_training_steps))
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=5, bias=True)
)
Training loop
def train_model(model, tr_dataloader):
    progress_bar = tqdm(range(num_training_steps))
    model.train()
    tr_losses = []

    # Training loop
    for epoch in range(num_epochs):
        total_loss = 0 

        for batch in tr_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            total_loss += loss.item()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            
        tr_losses.append(total_loss/len(tr_dataloader))
        
    plt.plot(tr_losses)
    plt.title("Training loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()
Evaluate
def evaluate_model(model, evl_dataloader):
    metric = Accuracy(task="multiclass", num_classes=5).to(device)
    model.eval()

    with torch.no_grad():
        for batch in evl_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            # Accumulate the predictions and labels for the metric
            metric(predictions, batch["labels"])

    # Compute the accuracy
    accuracy = metric.compute()
    print("Accuracy:", accuracy.item())
# train_model(model=model, tr_dataloader=train_dataloader)
# torch.save(model, 'bert-classification-model.pt')
loss_gpt.png

Loading the saved model
model.load_state_dict(torch.load('bert-classification-model.pt', map_location=device))
<All keys matched successfully>
evaluate_model(model, eval_dataloader)
Accuracy: 0.26499998569488525
 
Tuning a more complex model that can generate conversations between a human and an assistant using SFTtrainer.

Training a conversational model using SFTTrainer
How fine-tuning a decoder transformer using a specific dataset affects the quality of the generated responses in a question-answering task.

Load the train split of "timdettmers/openassistant-guanaco" dataset from Hugging Face:

dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")
dataset
README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]
Repo card metadata block was not found. Setting CardData to empty.
openassistant_best_replies_train.jsonl:   0%|          | 0.00/20.9M [00:00<?, ?B/s]
openassistant_best_replies_eval.jsonl: 0.00B [00:00, ?B/s]
Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]
Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]
Dataset({
    features: ['text'],
    num_rows: 9846
})
Load the pretrained causal model "facebook/opt-350m" along with its tokenizer from Hugging Face:

dataset[30]['text']
'### Human: What is the difference between open assistant and ChatGPT? Why should i use Open Assistant? And can you give me some examples of advanced chatbots that are similar to Open Assistant?### Assistant: First of all, a major difference is that Open Assistant is, as the name implies, open source. ChatGPT is closed source and might have more restrictions. \n\nOpen Assistant has less rules and restrictions than ChatGPT, and can be implemented into a multitude of applications. \n\nSome examples of similar chatbots are ChatGPT, Bing Chat, YouChat, Replit Ghostwriter, Character.ai, and Jasper, but there are many more.### Human: What is the difference in performance between the two models? Can you be more specific about the performance in conversational tasks, knowledge grounded question answering and in summarization?'
 
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]
pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]
generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]
tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]
vocab.json: 0.00B [00:00, ?B/s]
merges.txt: 0.00B [00:00, ?B/s]
special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]
Create instruction and response templates based on the train dataset format

instruction_template = "### Human:"
response_template = "### Assistant:"
Create a collator to curate data in the appropriate shape for training using "DataCollatorForCompletionOnlyLM":

collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)
Create an SFTTrainer object and pass the model as well as the dataset and collator

training_args = SFTConfig(
    output_dir="/tmp",
    num_train_epochs=10,
    learning_rate=2e-5,
    save_strategy="epoch",
    per_device_train_batch_size=2,  
    per_device_eval_batch_size=2,  
    max_seq_length=1024,
    do_eval=True
)

trainer = SFTTrainer(
    model,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",
    data_collator=collator,
)
Prompt the pretrained model with a specific question:

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=70)
print(pipe('''Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0]["generated_text"])
Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.

The term "monopsony" is used in the context of the "mono" (mono-economy) model. The term "mono-economy" is used in the context of the "mono-economy" model. The term "mono-economy" is used in the context of
Looks like the model is barely aware of what "monopsony" is in the context of economics.

print(pipe('''What is the difference between open assistant and ChatGPT? Why should i use Open Assistant? And can you give me 
some examples of advanced chatbots that are similar to Open Assistant?''')[0]["generated_text"])
What is the difference between open assistant and ChatGPT? Why should i use Open Assistant? And can you give me 
some examples of advanced chatbots that are similar to Open Assistant?

I'm not sure if Open Assistant is the same as ChatGPT, but I've used Open Assistant for a while and it's pretty good.

Open Assistant is a chatbot that can be used to communicate with other chatbots. It's a chatbot that can be used to communicate with other chatbots.

Chat
 
 
Training the model

# trainer.train()
Loading the tuned model that i trained on a GPU:

model.load_state_dict(torch.load('Assistant_model.pt', map_location=torch.device('cpu')))
<All keys matched successfully>
Checking how the tuned model performs in answering the same specialized question

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=70)
print(pipe('''Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use 
examples related to potential monopsonies in the labour market and cite relevant research.''')[0]["generated_text"])
Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.

The term "monopsony" in economics refers to the practice of controlling the working class by imposing a price on them. This can be seen as a form of economic control, but it can also be seen as a form of political control, as the price of a product can be influenced by the interests of the state.


 
print(pipe('''What is the difference between open assistant and ChatGPT? Why should i use Open Assistant? And can you give me 
some examples of advanced chatbots that are similar to Open Assistant?''')[0]["generated_text"])
What is the difference between open assistant and ChatGPT? Why should i use Open Assistant? And can you give me 
some examples of advanced chatbots that are similar to Open Assistant?

Open Assistant is a chatbot that uses a combination of AI and machine learning to generate text and generate responses. It is designed to be easy to use and to be able to understand and respond to user input. It is designed to be able to understand and respond to user input, and it can be used to generate text, generate responses,
 





import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

def create_summary_plots():
    """
    Generates and saves summary visualizations for the Transformer fine-tuning experiments.
    """
    plots_dir = "plots"
    if not os.path.exists(plots_dir):
        os.makedirs(plots_dir)
    
    sns.set_theme(style="whitegrid")
    performance_data = {
        'Method': [
            'Train from Scratch', 
            'Linear Probing', 
            'PEFT with LoRA', 
            'PEFT with Adapters', 
            'Full Fine-Tuning'
        ],
        'Accuracy': [83.0, 64.0, 69.2, 85.6, 86.0]
    }
    df_perf = pd.DataFrame(performance_data)
    
    plt.figure(figsize=(12, 7))
    ax1 = sns.barplot(
        data=df_perf, 
        x='Method', 
        y='Accuracy', 
        palette='viridis',
        order=df_perf.sort_values('Accuracy', ascending=False).Method 
    )
    
    for p in ax1.patches:
        ax1.annotate(
            f'{p.get_height():.1f}%', 
            (p.get_x() + p.get_width() / 2., p.get_height()), 
            ha='center', va='center', 
            fontsize=12, 
            color='black', 
            xytext=(0, 8), 
            textcoords='offset points'
        )
        
    ax1.set_title('Fine-Tuning Strategy Performance on IMDB Sentiment Analysis', fontsize=16, pad=20)
    ax1.set_xlabel('Fine-Tuning Method', fontsize=12)
    ax1.set_ylabel('Test Accuracy (%)', fontsize=12)
    ax1.set_ylim(0, 100)
    plt.xticks(rotation=15, ha="right")
    plt.figtext(0.5, 0.01, '*Note: LoRA was applied to a simpler baseline model, not the full Transformer.', ha="center", fontsize=9, style='italic')
    
    performance_plot_path = os.path.join(plots_dir, 'performance_comparison_imdb.png')
    plt.savefig(performance_plot_path, dpi=300, bbox_inches='tight')
    print(f"Performance comparison plot saved to {performance_plot_path}")
    plt.show()

    # Data extracted from your LoRA notebook
    efficiency_data = {
        'Method': ['Full Layer Update', 'PEFT with LoRA'],
        'Trainable Parameters': [12800, 456]
    }
    df_eff = pd.DataFrame(efficiency_data)

    plt.figure(figsize=(10, 6))
    ax2 = sns.barplot(data=df_eff, x='Trainable Parameters', y='Method', palette='mako', orient='h')

    # Use a log scale to visualize the huge difference
    ax2.set_xscale('log')
    
    # Add annotations
    for p in ax2.patches:
        width = p.get_width()
        ax2.text(
            width * 1.2, 
            p.get_y() + p.get_height() / 2, 
            f'{int(width):,}', 
            va='center', 
            fontsize=12
        )

    ax2.set_title('Parameter Efficiency: Full Update vs. LoRA', fontsize=16, pad=20)
    ax2.set_xlabel('Trainable Parameters (Log Scale)', fontsize=12)
    ax2.set_ylabel('')
    
    efficiency_plot_path = os.path.join(plots_dir, 'parameter_efficiency_comparison.png')
    plt.savefig(efficiency_plot_path, dpi=300, bbox_inches='tight')
    print(f"Parameter efficiency plot saved to {efficiency_plot_path}")
    plt.show()

if __name__ == '__main__':
    create_summary_plots()

